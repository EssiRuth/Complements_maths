---
title: "Projet"
author: "ruth"
date: "2025-03-07"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction

## Contexte

L’augmentation du nombre de cas de maladies cardiovasculaires à
l’échelle mondiale représente un défi majeur pour les systèmes de santé.
Parmi celles-ci, l’infarctus du myocarde figure comme l'une des
principales causes de mortalité. La prévention, via une détection
précoce des risques, constitue une voie cruciale d’intervention. Dans ce
contexte, l’analyse de données cliniques par des méthodes de
classification supervisée peut permettre de prédire efficacement le
risque d’infarctus chez un patient, et ainsi d'orienter plus rapidement
les stratégies de prise en charge.

## Objectif

Le présent projet vise à développer un modèle prédictif permettant
d’estimer, à partir de données médicales et comportementales, la
probabilité qu’un individu présente un risque d’infarctus. L’objectif
est double : D’une part, explorer les variables influentes dans la
prédiction du risque cardiovasculaire. D’autre part, évaluer les
performances de modèles de classification supervisée (arbre de décision,
k-NN et classification naive bayésienne) implémentés dans le langage R.

# I. Préparation des données

## 1. Présentation du jeu de données

Le jeu de données utilisé comprend 8763 individus et 26 variables, dont
une variable cible Heart.Attack.Risk indiquant si l’individu présente
(1) ou non (0) un risque d’infarctus. Les observations portent sur des
aspects cliniques (cholestérol, pression artérielle, triglycérides,
etc.), comportementaux (tabagisme, activité physique, régime
alimentaire) et démographiques (âge, sexe, revenu, pays de résidence).

```{r}
data=read.csv("~/Téléchargements/heart_attack_prediction_dataset.csv")
dim(data)
colnames(data)
```

Un aperçu des variables montre une diversité de types (quantitatif
continu, qualitatif binaire ou nominal, ordinal...), ce qui implique un
traitement différencié lors du prétraitement.

## 2. Description et pertinence des variables

Le jeu de données comprend 26 variables. Celles-ci couvrent des
informations cliniques, comportementales, démographiques et
géographiques. Afin de garantir la qualité de la modélisation, il est
essentiel d’identifier les variables les plus pertinentes, et d’en
exclure certaines qui peuvent introduire du bruit ou de la redondance.

Le tableau ci-dessous présente une description synthétique de chaque
variable, son type, ainsi que son niveau de pertinence pour le projet de
prédiction du risque d’infarctus :

```{r message=FALSE, warning=FALSE}
library(knitr)

variable_info <- data.frame(
  Variable = c("Patient.ID", "Age", "Sex", "Cholesterol", "Blood.Pressure", "Heart.Rate",
               "Diabetes", "Family.History", "Smoking", "Obesity", "Alcohol.Consumption",
               "Exercise.Hours.Per.Week", "Diet", "Previous.Heart.Problems",
               "Medication.Use", "Stress.Level", "Sedentary.Hours.Per.Day", "Income",
               "BMI", "Triglycerides", "Physical.Activity.Days.Per.Week",
               "Sleep.Hours.Per.Day", "Country", "Continent", "Hemisphere", "Heart.Attack.Risk"),
  
  Description = c("Identifiant unique du patient", "Âge du patient", "Sexe (Male/Female)",
                  "Taux de cholestérol total", "Tension artérielle (ex: 120/80)",
                  "Fréquence cardiaque", "Présence de diabète", 
                  "Antécédents familiaux de maladies cardiaques",
                  "Fumeur ou non", "Obésité", "Consommation d’alcool",
                  "Heures d’exercice par semaine", "Qualité de l’alimentation",
                  "Antécédents cardiaques", "Utilisation de médicaments", 
                  "Niveau de stress", "Heures assises par jour",
                  "Revenu annuel", "Indice de masse corporelle",
                  "Taux de triglycérides", "Jours d’activité physique/semaine",
                  "Heures de sommeil par jour", "Pays de résidence", 
                  "Continent de résidence", "Hémisphère", 
                  "Risque d’infarctus (0 ou 1)"),
  
  Type = c("Identifiant", "Quantitative", "Qualitative", "Quantitative", "Texte",
           "Quantitative", "Binaire", "Binaire", "Binaire", "Binaire", "Binaire",
           "Quantitative", "Qualitative", "Binaire", "Binaire", "Quantitative",
           "Quantitative", "Quantitative", "Quantitative", "Quantitative",
           "Quantitative", "Quantitative", "Qualitative", "Qualitative",
           "Qualitative", "Binaire"),
  
  Pertinence = c("Non pertinente", "Très pertinente", "Pertinente", "Pertinente",
                 " À transformer", " Pertinente", " Très pertinente",
                 " Pertinente", " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Très pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Non pertinente", " Non pertinente", " Non pertinente",
                 " Variable cible")
)

kable(variable_info, caption = "Tableau 1 : Description des variables et évaluation de leur pertinence", align = "l", booktabs = TRUE)

```

> Certaines variables comme `Patient.ID`, `Country`, `Continent` et
> `Hemisphere` seront écartées, car elles n’apportent pas de valeur
> informative dans la prédiction du risque cardiaque.

## 3. Typologie des variables

Les variables présentes dans le jeu de données peuvent être regroupées
en deux grandes catégories : **quantitatives** et **qualitatives**.
Cette distinction est importante, car elle détermine les techniques
statistiques et les méthodes de modélisation à employer.

### • Variables quantitatives

Il s’agit de variables **numériques continues ou discrètes** pouvant
faire l’objet de calculs statistiques classiques (moyenne, écart-type,
corrélation, etc.). Elles traduisent des mesures physiologiques ou
comportementales.

Les variables suivantes peuvent être considérées comme **quantitatives**
:

-   `Age` : Âge du patient\
-   `Cholesterol` : Taux de cholestérol\
-   `Heart.Rate` : Fréquence cardiaque\
-   `Exercise.Hours.Per.Week` : Heures d’exercice par semaine\
-   `Sedentary.Hours.Per.Day` : Heures passées assis par jour\
-   `Stress.Level` : Niveau de stress\
-   `Income` : Revenu annuel\
-   `BMI` : Indice de masse corporelle\
-   `Triglycerides` : Taux de triglycérides\
-   `Physical.Activity.Days.Per.Week` : Jours d’activité physique par
    semaine\
-   `Sleep.Hours.Per.Day` : Heures de sommeil par jour\
-   `Blood.Pressure` : À décomposer en `Systolic_BP` et `Diastolic_BP`
    pour une utilisation correcte

> Ces variables peuvent nécessiter un traitement préalable
> (normalisation, transformation logarithmique ou catégorisation) selon
> le modèle utilisé.

------------------------------------------------------------------------

### • Variables qualitatives

Ces variables sont de nature **catégorielle** (nominale ou binaire) et
doivent être **encodées** avant d’être intégrées dans un modèle de
machine learning. Elles représentent des caractéristiques
comportementales, médicales ou sociodémographiques.

Les variables suivantes relèvent de cette catégorie :

-   `Sex` : Sexe du patient\
-   `Diabetes` : Diabète (Oui/Non)\
-   `Family.History` : Antécédents familiaux de maladies cardiaques\
-   `Smoking` : Tabagisme\
-   `Obesity` : Obésité\
-   `Alcohol.Consumption` : Consommation d’alcool\
-   `Diet` : Qualité de l’alimentation (Healthy, Average, Unhealthy)\
-   `Previous.Heart.Problems` : Antécédents cardiaques\
-   `Medication.Use` : Prise de médicaments\
-   `Country` : Pays de résidence\
-   `Continent` : Continent de résidence\
-   `Hemisphere` : Hémisphère géographique\
-   `Heart.Attack.Risk` : **Variable cible**, binaire (0 = Pas de
    risque, 1 = Risque)

> Certaines de ces variables (comme `Country`, `Continent` ou
> `Hemisphere`) seront exclues de la modélisation en raison de leur
> faible lien explicite avec le risque d’infarctus. La distinction entre
> ces deux types est essentielle pour : - orienter les **prétraitements
> nécessaires** (encodage, normalisation, transformation), - choisir les
> **méthodes de visualisation exploratoire** adaptées, - **adapter les
> algorithmes de classification** à la structure des données.

```{r}
str(data)
```

## 4. La variable target

Notre variable target sera la variable Heart.Attack.Risk qui est une
variable binaire prenant la valeur 0 ou 1.

```{r}
target=as.factor(data$Heart.Attack.Risk)
class(target)
```

## 5. Nettoyage et vérification du data type

Les variables non pertinentes à l'analyse prédictive (comme Patient.ID,
Country, Continent, Hemisphere) ont été retirées, car elles n’apportent
pas d’information discriminante sur le risque d’infarctus.

De plus, la variable Blood.Pressure a été décomposée en deux variables
numériques distinctes : Systolic_BP (pression systolique) et
Diastolic_BP (pression diastolique), pour permettre une analyse fine.

```{r}
# Transformer Blood Pressure en deux colonnes numériques sans mutate()
data$Systolic_BP <- as.numeric(sub("/.*", "", data$Blood.Pressure))  # Extraire la pression systolique
data$Diastolic_BP <- as.numeric(sub(".*/", "", data$Blood.Pressure))  # Extraire la pression diastolique

# Supprimer la colonne originale Blood Pressure
data <- data[, !(names(data) %in% "Blood.Pressure")]

```

### Modification du type des variables

Avant d’appliquer les algorithmes de classification supervisée, il est
indispensable d’adapter le format des variables à leur nature et à
l’exigence des modèles utilisés. Les variables quantitatives doivent
être **converties en numériques** (`numeric`) pour permettre les calculs
statistiques, tandis que les variables qualitatives doivent être
**converties en facteurs** (`factor`) afin que les modèles puissent
reconnaître leurs modalités comme des catégories distinctes.

Ainsi, les variables représentant des **mesures continues** telles que
l’âge, le taux de cholestérol, le BMI ou encore le nombre d’heures de
sport sont converties en format `numeric`.\
En parallèle, les variables catégorielles comme le sexe, la présence de
diabète, le régime alimentaire ou les antécédents médicaux sont
converties en `factor`.

Une discrétisation de l’âge a également été réalisée pour créer une
variable `Age_cat`, classant les individus en tranches d’âge (`Jeune`,
`Middle-aged`, `Senior`, `Elderly`) afin de faciliter certaines analyses
exploratoires et comparatives.

Le code suivant présente ces différentes opérations de typage :

```{r}
# **Variables numériques continues (mesures)**
data$Age_cat <- cut(data$Age, 
                   breaks = c(0, 30, 50, 70, 100), 
                  labels = c("Jeune", "Middle-aged", "Senior", "Elderly"))

data$Cholesterol <- as.numeric(data$Cholesterol)
data$Heart.Rate <- as.numeric(data$Heart.Rate)
data$BMI <- as.numeric(data$BMI)
data$Triglycerides <- as.numeric(data$Triglycerides)
data$Exercise.Hours.Per.Week <- as.numeric(data$Exercise.Hours.Per.Week)
data$Sedentary.Hours.Per.Day <- as.numeric(data$Sedentary.Hours.Per.Day)
data$Stress.Level <- as.numeric(data$Stress.Level)
data$Income <- as.numeric(data$Income)
data$Physical.Activity.Days.Per.Week <- as.numeric(data$Physical.Activity.Days.Per.Week)
data$Sleep.Hours.Per.Day <- as.numeric(data$Sleep.Hours.Per.Day)

#  **Variables catégorielles (facteurs)**
data$Sex <- as.factor(data$Sex)
data$Smoking <- as.factor(data$Smoking)
data$Diabetes <- as.factor(data$Diabetes)
data$Family.History <- as.factor(data$Family.History)
data$Diet <- as.factor(data$Diet)
data$Previous.Heart.Problems <- as.factor(data$Previous.Heart.Problems)
data$Medication.Use <- as.factor(data$Medication.Use)
data$Alcohol.Consumption <- as.factor(data$Alcohol.Consumption)
data$Obesity <- as.factor(data$Obesity)

str(data) 

```

## 6. Séparation des variables quantitatives et qualitatives

Dans le but de faciliter l’analyse statistique et la modélisation, les
variables du jeu de données ont été regroupées en deux sous-ensembles
distincts :

-   **Les variables quantitatives**, qui correspondent à des mesures
    numériques continues, sont destinées à des analyses statistiques
    classiques (statistiques descriptives, corrélations, boxplots,
    etc.).
-   **Les variables qualitatives**, qui regroupent les catégories (sexe,
    antécédents, comportement, etc.), feront l’objet d’analyses de
    fréquence et de visualisations spécifiques (diagrammes en barres,
    tables de contingence, etc.).

Ce regroupement permet une gestion plus efficace des données lors des
étapes suivantes du projet, notamment lors de l’analyse exploratoire, de
l’encodage, et de l’entraînement des modèles.

Le code suivant permet d’extraire ces deux sous-ensembles à l’aide du
package `dplyr` :

```{r}
library(dplyr)
quantitative_vars <- select(data, Cholesterol, Heart.Rate, BMI, Triglycerides, 
                            Systolic_BP, Diastolic_BP, Exercise.Hours.Per.Week, Sedentary.Hours.Per.Day, 
                            Stress.Level, Physical.Activity.Days.Per.Week, Income,
                            Sleep.Hours.Per.Day)


qualitative_vars <- select(data, Age_cat, Sex, Smoking, Diabetes, Obesity, Family.History, 
                           Diet, Previous.Heart.Problems, Alcohol.Consumption, Medication.Use)
str(qualitative_vars)
str(quantitative_vars)


```

# II. Analyse exploratoire

## 1. Analyse univariée

### Variables quantitatives

Un résumé statistique a été effectué afin d’identifier les
distributions, valeurs aberrantes et échelles des variables continues :

```{r}
summary(quantitative_vars)
```

Les statistiques descriptives des variables quantitatives montrent que
les données sont globalement cohérentes et réalistes, sans valeurs
aberrantes majeures. On peut en dégager les observations suivantes :

-   **Cholestérol** : les valeurs varient de **120 à 400 mg/dL**, avec
    une moyenne de **260 mg/dL**, indiquant une population globalement
    en situation d’**hypercholestérolémie modérée à sévère**.

-   **Fréquence cardiaque (`Heart.Rate`)** : entre **40 et 110
    battements par minute**, avec une moyenne à **75 bpm**, ce qui reste
    dans les normes physiologiques générales.

-   **IMC (`BMI`)** : varie entre **18 et 40**, avec une moyenne
    d’environ **29**, ce qui correspond à une population en **surpoids
    voire obèse**, ce qui est attendu dans ce contexte médical.

-   **Triglycérides** : valeurs allant de **30 à 800 mg/dL**, avec une
    moyenne de **417 mg/dL**. Bien que la valeur maximale soit élevée,
    elle reste possible médicalement, et reflète un fort risque
    métabolique chez une partie de la population.

-   **Pression artérielle systolique et diastolique** : les valeurs
    moyennes sont de **135 mmHg** pour la systolique et **85 mmHg** pour
    la diastolique, proches des seuils de l’hypertension. Les valeurs
    minimales et maximales sont plausibles.

-   **Exercise.Hours.Per.Week** et **Physical.Activity.Days.Per.Week** :
    montrent une forte variabilité. La moyenne d’heures d’exercice est
    autour de **10 heures**, avec une distribution allant de **0 à près
    de 20 heures**, ce qui indique une **hétérogénéité importante dans
    les habitudes d’activité physique**.

-   **Sedentary.Hours.Per.Day** : varie de **0 à près de 12 heures**,
    avec une moyenne proche de **6 heures**, ce qui est élevé, mais
    réaliste pour une population potentiellement à risque.

-   **Stress.Level** : notée entre **1 et 10**, avec une moyenne autour
    de **5.5**, traduisant un **niveau de stress perçu modéré à élevé**.

-   **Sleep.Hours.Per.Day** : moyenne autour de **7 heures**, avec une
    distribution raisonnable (entre 4 et 10 heures), cohérente avec les
    recommandations de sommeil pour adultes.

-   **Income** : exprimé en unités monétaires (non spécifiées), varie de
    **\~80 000 à \~300 000**, avec une moyenne de **158 263**. Cette
    variable présente une distribution large, probablement asymétrique.

### Variables qualitatives

Le nombre d’observations tombées dans chacune des catégories

```{r}
sapply(qualitative_vars, table)
```

L’analyse des variables qualitatives met en évidence des répartitions
globalement équilibrées pour la majorité d’entre elles, tout en
soulignant certains déséquilibres notables qui reflètent le profil à
risque cardiovasculaire de la population étudiée.

La variable `Age_cat`, répartie en quatre tranches d’âge (Jeune,
Middle-aged, Senior, Elderly), montre une bonne répartition, légèrement
concentrée autour des personnes d’âge moyen et senior. Cela garantit une
représentation suffisante de chaque classe d’âge dans l’analyse.

La variable `Sex` présente un déséquilibre important, avec près de 70%
d’hommes. Cette surreprésentation du sexe masculin devra être prise en
compte dans l’interprétation des résultats, car elle peut influencer la
prédiction du risque.

Concernant le mode de vie et les antécédents médicaux :

-   La majorité des individus sont **fumeurs** (près de 90%),
    **diabétiques** (65%) et **obèses** (50%). Ces proportions très
    élevées soulignent le caractère à haut risque de la population. - La
    variable `Family.History`, qui indique la présence d’antécédents
    familiaux de maladies cardiaques, est bien équilibrée (≈ 50/50),
    tout comme `Previous.Heart.Problems`, `Medication.Use` et `Obesity`,
    permettant une modélisation comparative efficace.

-   Les types de régime (`Diet`) sont également répartis de manière
    homogène entre "Healthy", "Average" et "Unhealthy", ce qui permet
    d’étudier leur impact potentiel sans biais de distribution. - Enfin,
    `Alcohol.Consumption` révèle que 60% des individus consomment de
    l’alcool, contre 40% qui n’en consomment pas, ce qui constitue une
    base statistique suffisamment variée pour en évaluer les effets.
    Toutes cesvariables sont conservées dans l’analyse car elles sont
    soit bien réparties, soit médicalement pertinentes, et permettent
    d’alimenter efficacement la modélisation du risque d’infarctus.

### Variable Target

La variable cible est `Heart.Attack.Risk`

```{r}

table(target)
```

On observe 5624 observations dans la classe 0 (soit 64,2%) et 3139 dans
la classe 1 (35,8%). Cette répartition présente un léger déséquilibre,
mais reste suffisamment équilibrée pour permettre une modélisation
fiable des deux classes.

## 2. Analyse graphique

### 2.1 Analyse des variables quantitatives

```{r, out.width="50%", fig.align = "center"}
for (col in colnames(quantitative_vars)){
  boxplot(quantitative_vars[[col]], main=paste("Boxplot", col), col="lightblue", breaks=30)
}
```

***Interprétation***

-   Cholesterol : Q1 = 192, Médiane = 259, Q3 = 330 et Pas de valeurs
    aberrantes =\> 50 % des données se situent entre 192 et 330. La
    médiane est de 259 mg/dL. L’ensemble des observations reste dans la
    plage normale définie par les moustaches.

-   Heart Rate : Q1 = 57, Médiane = 75, Q3 = 93 , Min = 40, Max = 110 et
    Pas de valeurs extrêmes =\> La moitié des individus ont une
    fréquence cardiaque entre 57 et 93 bpm. La médiane est à 75. Aucune
    valeur n’est considérée comme aberrante selon les critères du
    boxplot.

-   Exercise Hours Per Week: Q1 ≈ 5, Médiane ≈ 10, Q3 ≈ 15 ,Min ≈ 0, Max
    ≈ 20 et Pas de points au-delà des moustaches =\> 50 % des individus
    font entre 5 et 15 heures d’exercice par semaine. La médiane est
    de 10. La distribution est régulière et aucune valeur n’est jugée
    extrême.

-   Stress Level : Q1 = 3, Médiane = 5, Q3 = 8 et Min = 1, Max = 10 Pas
    de valeurs extrêmes =\> Le niveau de stress est compris entre 3 et 8
    pour la moitié des individus, avec une médiane de 5. Les moustaches
    couvrent l’ensemble des données, sans valeur aberrante.

-   Sedentary Hours Per Day : Q1 ≈ 3, Médiane ≈ 6, Q3 ≈ 9,Min ≈ 0, Max ≈
    12 et Pas de valeur extrême =\> Les heures passées assis par jour
    sont comprises entre 3 et 9 pour la moitié des individus. La médiane
    est à 6. Tous les individus restent dans les limites des moustaches.

-   Triglycerides: Q1 = 225.5, Médiane = 417, Q3 = 612, Min = 30, Max =
    800 et pas de valeur aberrante détectée =\> Bien que les valeurs
    varient fortement, la distribution respecte les limites définies par
    les moustaches. Aucune valeur extrême détectée malgré un écart
    important.

-   Physical Activity Days Per Week : Q1 = 2, Médiane = 3, Q3 = 5,Min =
    0, Max = 7 et aucun point hors des moustaches =\> 50 % des individus
    font entre 2 et 5 jours d’activité physique par semaine. La médiane
    est de 3 jours. Toutes les valeurs sont incluses dans les
    moustaches, donc aucune valeur extrême n’est présente.

-   Sleep Hours Per Day : Q1 = 5, Médiane = 7, Q3 = 9, Min = 4, Max = 10
    et Aucune valeur hors de l’intervalle attendu =\> La moitié des
    individus dorment entre 5 et 9 heures par jour. La médiane est de 7
    heures. La distribution est équilibrée, sans valeur aberrante
    détectée.

-   Heart Attack Risk : Variable binaire : Min = 0, Max = 1, Médiane =
    0, Q1 = 0, Q3 = 1 et Rien à détecter ici car ce n’est pas une
    variable continue =\> Cette variable prend uniquement les valeurs 0
    ou 1. Le boxplot n’est pas pertinent ici car la variable est
    catégorique.

-   systolic_BP : Q1 = 112, Médiane = 135, Q3 = 158, Min = 90, Max = 180
    et Toutes les valeurs sont dans les moustaches =\> La moitié des
    pressions systoliques se situent entre 112 et 158 mmHg, avec une
    médiane de 135. Aucune valeur extrême n’est présente selon la règle
    de 1,5×IQR.

### 2.2 Variables qualitatives: barplot

```{r, out.width="50%", fig.align = "center"}
for (col in colnames(qualitative_vars)){
  barplot(table(qualitative_vars[[col]]), main=paste("Histogramme de", col), col="lightgreen")
}
# Afficher les effectifs de chaque modalité pour chaque variable qualitative
for (col in colnames(qualitative_vars)) {
  cat("====", col, "====\n")
  print(table(qualitative_vars[[col]]))
  cat("\n\n")
}
```

**Interprétation**

-   Age_cat : La majorité de l’échantillon est composée de personnes
    d’âge moyen à élevé : 2441 Middle-aged, 2346 Senior et 2361 Elderly.
    La catégorie Jeune est sous-représentée (1615), ce qui montre un
    biais vers les tranches d’âge supérieures.

-   Sex : L’échantillon est fortement déséquilibré : 6111 hommes contre
    2652 femmes. Ce déséquilibre de genre devra être pris en compte dans
    l’analyse.

-   Smoking : 7859 individus déclarent fumer contre seulement 904
    non-fumeurs. Cela montre une très forte proportion de fumeurs dans
    la base.

-   Diabetes : 5716 individus sont atteints de diabète, contre 3047
    non-diabétiques. Le diabète est donc très présent dans cette
    population.

-   Obesity : La répartition est équilibrée : 4394 personnes sont en
    situation d’obésité contre 4369 qui ne le sont pas. Cette variable
    permet une analyse comparative directe.

-   Family.History : Les antécédents familiaux de maladies cardiaques
    concernent 4320 personnes, tandis que 4443 n’en ont pas. La
    distribution est globalement équilibrée.

-   Previous.Heart.Problems : 4345 individus présentent des antécédents
    de problèmes cardiaques, contre 4418 qui n’en ont pas. La
    répartition est presque équilibrée, ce qui permettra d’étudier
    correctement l’impact de ces antécédents sur le risque d’infarctus.

-   Medication.Use : 4367 personnes déclarent prendre un traitement
    médicamenteux, contre 4396 qui n’en prennent pas. Cette variable est
    également équilibrée, ce qui facilite les comparaisons entre les
    deux groupes.

-   Alcohol.Consumption : 5241 individus consomment de l’alcool, tandis
    que 3522 n’en consomment pas. Une majorité est donc consommatrice,
    ce qui peut représenter un facteur de risque comportemental
    important à analyser.

### 2.3 Variables quantitatives et target

```{r}
for(i in 1:ncol(quantitative_vars)){
  boxplot(quantitative_vars[,i] ~ target, main=paste("Boxplot de", colnames(quantitative_vars)[i]),
          col = c("lightgreen", "lightblue"), xlab = "Risque d'infarctus")
}
```

#### Interprétation

-   Cholesterol : La médiane et la dispersion sont très proches entre
    les deux groupes. Il n’y a pas de valeur aberrante visible. Le
    cholestérol seul ne semble pas un bon discriminateur du risque.

-   Heart.Rate : Les médianes sont similaires (autour de 75-80) et la
    dispersion est équivalente. Aucune différence notable n’apparaît
    visuellement entre les deux groupes.

-   BMI : La distribution de l’IMC est très proche entre les deux
    groupes, avec une médiane légèrement supérieure chez les individus à
    risque. Cela pourrait indiquer un lien modéré entre surpoids et
    risque cardiaque, à confirmer statistiquement.

-   Triglycerides : Les médianes sont un peu plus élevées dans le groupe
    à risque, mais les distributions restent très proches. Aucun écart
    important ou valeur extrême n’est détecté.

-   Systolic_BP et Diastolic_BP : Les distributions sont presque
    identiques entre les groupes. Cela suggère que la pression
    artérielle n’est pas significativement différente selon le risque,
    du moins visuellement.

-   Exercise.Hours.Per.Week : Aucune différence visuelle marquée n’est
    observée. La médiane est proche, et les étendues sont équivalentes.
    Le niveau d’exercice hebdomadaire n’apparaît pas comme un facteur
    discriminant clair ici.

-   Stress.Level : Les deux groupes ont des distributions similaires.
    Bien que le stress soit souvent évoqué comme un facteur de risque,
    il ne ressort pas ici de manière nette.

-   Sedentary.Hours.Per.Day : Les individus à risque semblent avoir une
    légère tendance à passer un peu plus de temps assis, mais la
    différence reste faible.

-   Physical.Activity.Days.Per.Week : On observe une distribution
    légèrement plus étendue dans le groupe à risque, avec une médiane
    équivalente. Il n’y a pas de signe fort de différence ici non plus.

-   Income : Les revenus semblent globalement similaires entre les
    groupes. La médiane et la dispersion sont quasi identiques,
    suggérant peu ou pas d’effet du revenu sur le risque dans ce jeu de
    données.

-   Sleep.Hours.Per.Day : La médiane est identique entre les deux
    groupes (environ 7 heures). Les distributions sont semblables et
    symétriques, sans valeurs extrêmes détectées. Le sommeil ne semble
    donc pas différencier clairement les individus à risque des autres.

### 2.4. Variables qualitatives et target

```{r}
#library(ggplot2)
for(i in 1:ncol(qualitative_vars)){
var1 = as.character(qualitative_vars[,i])
var1 = as.factor(var1)
counts = NULL # il nous faut le nombre de passagers dans chacune des catégories
# définies par la variable quantitative et la target
for(j in levels(var1)){
for(k in levels(target)){
counts = rbind(counts, c(var1 = j, target = k,
patient_num = sum(target == k & var1 == j)))

}}
counts = as.data.frame(counts)
counts$var1 = as.factor(counts$var1)
counts$target = as.factor(counts$target)
counts$patient_num = as.numeric(counts$patient_num)
xlab_i = paste0(colnames(qualitative_vars)[i])
print(ggplot(data = counts, aes(x = var1, y = patient_num, fill = target)) +
geom_bar(stat = "identity", color="black") +
theme_minimal() + labs(x = xlab_i, fill = " Atteinte de risque cardiaque",
y = "Nombre de patients"))
}

```

#### Interprétation

-   La variable catégorisée Age_cat montre que le nombre de patients à
    risque d’infarctus varie selon l’âge. Bien que les groupes
    "Middle-aged", "Senior" et "Elderly" soient proches en taille, le
    groupe "Jeune" présente une proportion relativement élevée de cas à
    risque, ce qui peut surprendre. Cela pourrait refléter d'autres
    facteurs aggravants chez les jeunes (mode de vie, hérédité...) et
    suggère que l’âge seul ne suffit pas à prédire le risque, d’où
    l’intérêt de combiner cette variable avec d'autres dans la
    modélisation.

-   Répartition de Sex selon le risque : Le nombre d’hommes dans le
    groupe à risque est nettement supérieur à celui des femmes.Le sexe
    masculin apparaît donc clairement corrélé au risque cardiaque dans
    ce jeu de données.

-   Répartition de Smoking selon le risque : La majorité des fumeurs se
    trouve dans le groupe à risque, tandis que les non-fumeurs sont peu
    présents dans ce groupe.Le tabagisme est ici un facteur fortement
    associé au risque, avec un potentiel explicatif élevé.

-   Répartition de Diabetes selon le risque : Les diabétiques sont
    nettement plus nombreux dans le groupe à risque que les
    non-diabétiques.Cette variable est donc fortement liée au risque de
    crise cardiaque et sera déterminante dans la modélisation.

-   Répartition de Obesity selon le risque : Les effectifs sont très
    similaires dans les deux groupes, que ce soit pour les obèses ou
    non.L’obésité n’apparaît pas ici comme un facteur discriminant net,
    bien qu’elle puisse jouer un rôle indirect.

-   Répartition de Family.History selon le risque : Légère
    surreprésentation des antécédents familiaux dans le groupe à risque,
    mais la différence reste modérée. Cette variable pourrait donc être
    informative, mais faiblement discriminante seule.

-   Répartition de Diet selon le risque d’infarctus : Les effectifs sont
    très proches entre les groupes, quelle que soit la qualité du régime
    alimentaire. La variable Diet ne montre pas de tendance marquée
    entre alimentation et risque cardiaque. Son pouvoir prédictif semble
    faible lorsqu’elle est prise isolément.

-   Répartition de Previous.Heart.Problems selon le risque : Les
    patients ayant eu des antécédents cardiaques sont légèrement plus
    nombreux dans le groupe à risque. Cette variable est donc
    pertinente, mais la différence reste modérée.

-   Répartition de Medication.Use selon le risque : Les proportions sont
    quasiment identiques entre les deux groupes. La prise de médicament
    ne permet pas de discriminer clairement les patients à risque dans
    ce jeu de données.

-   Répartition de Alcohol.Consumption selon le risque : La consommation
    d’alcool est nettement plus élevée chez les patients à risque. Cette
    variable est donc fortement associée au risque d’infarctus, et
    mérite d’être prise en compte dans la modélisation.

## 3. Analyse multivariée

### 3.1 Gestion des valeurs manquantes

Avant de procéder à l'analyse multivariée, nous avons vérifié la
présence de valeurs manquantes dans nos variables quantitatives et
qualitatives. Cette étape est essentielle, car les valeurs manquantes
peuvent fausser les analyses et nécessitent généralement des méthodes
d'imputation ou d'exclusion.

```{r}
# verification de valeurs manquantes
colSums(is.na(qualitative_vars))
colSums(is.na(quantitative_vars))

```

Après vérification, nous avons constaté que notre base de données **ne
contient aucune valeur manquante**. Ainsi, aucune action corrective
(imputation ou suppression) n’est requise, ce qui simplifie l’analyse à
venir et garantit l’intégrité des données.

### 3.2 Analyse Graphique

#### Analyse des variables quantitatives

```{r}

library(ggplot2)
library(reshape2)

# Calcul de la matrice de corrélation
cor_mat <- cor(quantitative_vars, use = "complete.obs")
cor_mat

# Transformation en format long
melted_cor_mat <- melt(cor_mat)

# Création du heatmap amélioré
ggplot(data = melted_cor_mat, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", limits =c(-1,1)) +
  labs(title = "Correlation Heatmap", x = "", y = "", fill = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

**Interprétation** **Corrélations positives**

-   Triglycérides & Systolic_BP (0.0051) et Triglycérides & Diastolic_BP
    (0.0005) : Très faible corrélation positive entre les triglycérides
    et la pression artérielle. Les valeurs sont proches de 0, donc pas
    significatives.

-   Cholestérol & BMI (0.0173) : Une légère corrélation positive entre
    l’indice de masse corporelle (BMI) et le taux de cholestérol. une
    prise de poids peut être associée à une augmentation du cholestérol.

**Corrélations négatives**

-   Stress Level & Physical Activity Days Per Week (-0.0074) : Une
    légère corrélation indiquant que plus une personne fait de
    l’activité physique, moins elle ressent de stress. Cependant, la
    valeur est très proche de 0, donc cette relation est faible.

-   Triglycérides & Physical Activity Days Per Week (-0.0075) : Indique
    que les personnes qui pratiquent plus d’activité physique ont
    tendance à avoir un taux de triglycérides légèrement plus bas.

**Conclusion Générale** On observe aucune ucune relation forte entre les
variables : Toutes les corrélations sont proches de 0, ce qui signifie
qu’aucune variable ne semble influencer fortement une autre dans cet
ensemble de données. Ce qui suggère que toutes les variables sont
essentielles(pas de redondance dans les variables) pour prédire le
risque d'infarctus.

# III Modèles d'évaluation

## 1. Methodes des k_plus proches voisins

#### Transformation des variables explicatives

```{r}
library(caret) # chargement de la librairie caret
```

-   Variables quantitatives

```{r}
# Normalisation des variables quantitatives
Proc = preProcess(quantitative_vars, method = "range")

# On regroupe les nouvelles variables normalisées dans quantitative_vars_mnormed
quantitative_vars_mmnormed = predict(Proc, quantitative_vars)

# Affiche les premières lignes pour vérifier
head(quantitative_vars_mmnormed)
```

-   Variables qualitatives

```{r}
#  Conversion de toutes les colonnes en facteur (base R)
qualitative_vars[] <- lapply(qualitative_vars, as.factor)

# Création du modèle de transformation avec dummyVars de la librairie caret
dummy_model <- dummyVars(" ~ .", data = qualitative_vars)

# On regroupe les nouvelles variables codées dans qual_dummies
qual_dummies <- as.data.frame(predict(dummy_model, newdata = qualitative_vars))

# Vérifie si le codage a bien été faite
str(qual_dummies)
```

#### Les variables explicatives

Data frame `predictive_vars` contenant les variables transformées

```{r}
predictive_vars=cbind(quantitative_vars_mmnormed, qual_dummies)
```

### 1.1 Méthode de Holdout

#### Partition du Jeu de Données (Holdout Split)

Dans cette étape, nous réalisons une séparation aléatoire de notre jeu
de données en deux sous-ensembles : un ensemble d’apprentissage (Train)
et un ensemble de test (Test).

Pour assurer la reproductibilité des résultats, nous commençons par
fixer une graine aléatoire avec `set.seed(1237)`. Cela garantit que la
sélection des observations reste la même à chaque exécution du code.

Nous déterminons ensuite le nombre total d’observations dans notre jeu
de données à l’aide de nrow(predictive_vars). Puis, nous utilisons la
fonction sample() pour tirer aléatoirement 80 % des observations qui
constitueront l’ensemble d’apprentissage.

Les données sont ensuite partitionnées comme suit :

predictive_vars_train et target_train : contiennent les variables
prédictives et la cible pour l’ensemble d’apprentissage (80 % des
données).

predictive_vars_test et target_test : contiennent les observations
restantes (20 % des données), qui serviront à évaluer la performance du
modèle.

Ce découpage est essentiel pour éviter le surajustement et garantir que
le modèle apprend sur un ensemble d’entraînement tout en étant évalué
sur un jeu de test indépendant.

```{r}
set.seed(1237) #pour la reproductibilité 
nobs <- nrow(predictive_vars)
inTrain <- sample(1:nobs, size = round(0.8 * nobs), replace = FALSE)

predictive_vars_test    = predictive_vars[-inTrain,]
target_test             = target[-inTrain]
predictive_vars_train   = predictive_vars[inTrain,]
target_train            = target[inTrain]
```

Afin d’évaluer efficacement la performance du modèle et d’optimiser ses
hyperparamètres, nous effectuons une partition du jeu de données en
trois sous-ensembles :

-   **Ensemble d’apprentissage (Train - 80 %)** : utilisé pour entraîner
    le modèle.\
-   **Ensemble de validation (Validation - 10 %)** : permet d’ajuster
    les hyperparamètres et d’éviter le surajustement.\
-   **Ensemble de test (Test - 10 %)** : sert à évaluer la performance
    finale du modèle sur des données totalement indépendantes.

Pour garantir la reproductibilité de la répartition des observations,
nous fixons une graine aléatoire. Ensuite, nous sélectionnons
aléatoirement **10 % des observations pour l’ensemble de test** et **10
% pour l’ensemble de validation**, en veillant à ce que ces ensembles ne
se chevauchent pas. Le reste des données est attribué à l’ensemble
d’apprentissage (80 %).

Enfin, les variables cibles des ensembles d’apprentissage, de validation
et de test sont converties en **facteurs**, ce qui est essentiel pour
garantir un traitement correct des données catégoriques dans les modèles
supervisés.

Cette séparation en trois ensembles permet d’obtenir une évaluation plus
robuste et d’améliorer la généralisation du modèle en évitant un
sur-apprentissage.

```{r}
set.seed(1237)
inTest = sample(1:nobs, size = round(0.1 * nobs), replace = FALSE)
notinTest = (1:nobs)[-inTest]
inValidation = sample(notinTest, size = round(0.1 * nobs), replace = FALSE)
c(inTest, inValidation)

# Création des datasets
predictive_vars_test       = predictive_vars[inTest,]
target_test                = target[inTest]

predictive_vars_validation = predictive_vars[inValidation,]
target_validation          = target[inValidation]

predictive_vars_train      = predictive_vars[inTrain,]
target_train               = target[inTrain]

```

#### Sélection de l’Hyperparamètre k

Dans cette étape, nous cherchons à optimiser le nombre de voisins **k**
pour la méthode des k plus proches voisins (kNN). L’hyperparamètre **k**
joue un rôle essentiel dans la performance du modèle :\
- Un **k trop petit** risque d'entraîner un modèle trop sensible aux
variations locales des données, pouvant causer du **surajustement**.\
- Un **k trop grand** peut lisser excessivement les prédictions et
réduire la capacité du modèle à capturer des motifs locaux dans les
données.

Nous testons donc différentes valeurs de **k** (de 1 à 20) et évaluons
les performances du modèle sur l’ensemble de validation en utilisant
plusieurs métriques :\
- **Accuracy** : mesure globale des prédictions correctes.\
- **Recall (Sensibilité)** : proportion des instances positives
correctement identifiées.\
- **Precision** : proportion des prédictions positives qui sont
réellement correctes.

Pour chaque valeur de **k**, nous utilisons l’algorithme **kNN** pour
prédire les classes des observations dans l’ensemble de validation, puis
nous évaluons les performances du modèle avec une **matrice de
confusion**.

Enfin, nous regroupons les résultats sous forme d’un tableau, ce qui
nous permet d’identifier la valeur optimale de **k** en fonction des
performances observées.

#### Sélection de l’Hyperparamètre k

Dans cette étape, nous cherchons à optimiser le nombre de voisins **k**
pour la méthode des k plus proches voisins (kNN). L’hyperparamètre **k**
joue un rôle essentiel dans la performance du modèle :\
- Un **k trop petit** risque d'entraîner un modèle trop sensible aux
variations locales des données, pouvant causer du **surajustement**.\
- Un **k trop grand** peut lisser excessivement les prédictions et
réduire la capacité du modèle à capturer des motifs locaux dans les
données.

Nous testons donc différentes valeurs de **k** (de 1 à 20) et évaluons
les performances du modèle sur l’ensemble de validation en utilisant
plusieurs métriques :\
- **Accuracy** : mesure globale des prédictions correctes.\
- **Recall (Sensibilité)** : proportion des instances positives
correctement identifiées.\
- **Precision** : proportion des prédictions positives qui sont
réellement correctes.

Pour chaque valeur de **k**, nous utilisons l’algorithme **kNN** pour
prédire les classes des observations dans l’ensemble de validation, puis
nous évaluons les performances du modèle avec une **matrice de
confusion**.

Enfin, nous regroupons les résultats sous forme d’un tableau, ce qui
nous permet d’identifier la valeur optimale de **k** en fonction des
performances observées.

```{r, results = 'hide'}

library(class) ## chargement du jeu de donnees
library(caret) ### pour les metriques d'evaluation
```

```{r}

# Définition de la grille des valeurs de K à tester
# Convertir en data.frame
predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_validation <- as.data.frame(predictive_vars_validation)
KGrid         = 1:20
accuracy_val  = NULL
recall_val    = NULL
precision_val = NULL


# Boucle pour tester chaque valeur de K
for(k in KGrid){
  
  # Prédiction avec kNN
  target_pred = knn(train = predictive_vars_train, 
                    test = predictive_vars_validation, 
                    cl = target_train, 
                    k = k) 
  
  # Conversion en facteur pour correspondre à target_validation
  target_pred = as.factor(target_pred)
# Vérifier et harmoniser les niveaux des facteurs
target_pred = factor(target_pred, levels = levels(target_validation))

  
  # Calcul des métriques avec confusionMatrix()
  cm = confusionMatrix(target_pred, target_validation, positive = "1")

  accuracy_val  = c(accuracy_val, cm$overall["Accuracy"])
  recall_val    = c(recall_val, cm$byClass["Sensitivity"])  # Sensitivity = Recall
  precision_val = c(precision_val, cm$byClass["Precision"])
}

# Afficher les résultats sous forme de tableau
results = data.frame(K = KGrid, Accuracy = accuracy_val, Recall = recall_val, Precision = precision_val)
print(results)

```

> Les résultats obtenus montrent que l’accuracy diminue progressivement
> avec k, atteignant un maximum d’environ 0.90 pour k = 1. Cependant,
> cette diminution de l'accuracy s'accompagne d'une baisse du recall,
> indiquant que le modèle devient plus conservateur à mesure que k
> augmente, identifiant moins de cas positifs. En revanche, la précision
> tend à s’améliorer légèrement avec un k plus élevé.

On observe ainsi un compromis entre recall et précision :

Un faible k (ex: k = 1-3) permet d’identifier plus de cas positifs
(meilleur recall), mais avec un risque de surajustement aux données
d'entraînement.

Un k plus élevé (ex: k \> 10) stabilise le modèle et augmente la
précision, mais réduit la sensibilité aux cas positifs.

Afin d’obtenir un équilibre entre précision, recall et stabilité du
modèle, nous avons opté pour k = 4.

Avec k = 4, nous obtenons une accuracy de 0.68,

Un recall de 0.39, assurant une détection correcte d’une bonne partie
des cas positifs,

Une précision de 0.595, permettant de limiter les fausses alertes.

### Les différentes métriques en fonction des valeurs de K étudiées

Afin de mieux visualiser l'impact du choix de **k** sur les performances
du modèle, nous avons représenté graphiquement l'évolution des
différentes métriques(**accuracy**, **recall** et **precision**) en
fonction des valeurs de **k** testées.

```{r, out.width="50%", fig.align = "center", eval = FALSE}
plot(KGrid, accuracy_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Accuracy", col="purple")
plot(KGrid, recall_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Recall", col="red")
plot(KGrid, precision_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Precision", col="blue")
```

#### Interpretation des graphiques

-   Accuracy (précision globale du modèle) : représentée en violet, elle
    atteint son maximum pour k = 1, puis diminue progressivement avant
    de se stabiliser autour de k = 20. Cela reflète le compromis entre
    un modèle très flexible (faible k) et un modèle plus stable mais
    moins précis (grand k).

-   Recall (sensibilité) : en rouge, cette métrique décroît avec
    l’augmentation de k, indiquant que le modèle détecte de moins en
    moins de cas positifs à mesure que k augmente. Un faible k favorise
    donc la détection des cas positifs, mais au risque d'une plus grande
    variabilité des résultats.

-   Précision : en bleu, elle tend à s’améliorer légèrement avec k, ce
    qui signifie que les prédictions positives deviennent plus fiables,
    bien que cela se fasse au détriment du recall. Un k plus élevé
    privilégie donc des prédictions plus précises mais moins nombreuses.

#### Performance du classifieur final

Pour évaluer la performance de notre modèle, nous avons entraîné un
classifieur k-plus proches voisins (k-NN) avec k = 4, valeur choisie
pour assurer un bon équilibre entre recall et précision.

```{r, results = 'hide', eval = FALSE}
rbind(predictive_vars_train, predictive_vars_validation)
c(target_train, target_validation)
```

```{r, eval = FALSE}
set.seed(1237)
target_pred = knn(train = rbind(predictive_vars_train, predictive_vars_validation),
test = predictive_vars_test,
cl = c(target_train, target_validation),
k = 4)
accuracy_test = mean(target_pred == target_test)
recall_test = recall(data = target_pred, target_test,
relevant = "1")
precision_test = precision(target_pred, target_test,
relevant = "1")
confusionMatrix(target_pred, target_test)$table
accuracy_test
recall_test
precision_test

```

> L'évaluation sur l’ensemble de test donne les résultats suivants :

-   Accuracy : 69.40 % :Le modèle a correctement prédit le risque chez
    environ 7 patients sur 10. Bien que ce score soit supérieur au taux
    de hasard (64,2 % correspondant à la classe majoritaire), il reste
    modeste pour une application médicale, où la fiabilité doit être
    maximale.

-   Recall : 46.17 % : Parmi tous les patients effectivement à risque
    d’infarctus, le modèle en a correctement identifié moins d’un sur
    deux. Cela signifie qu’il laisse passer plus de 50 % des patients à
    risque, ce qui est problématique dans un contexte de santé publique,
    car le coût d’un faux négatif (non détecté) peut être critique.

-   Précision : 59.42 % : Sur tous les patients prédit comme étant à
    risque, seulement 59 % le sont réellement. Cela reflète une présence
    non négligeable de faux positifs, c’est-à-dire des patients
    faussement signalés comme à risque, ce qui pourrait entraîner des
    examens inutiles ou inquiéter à tort.

Ces résultats traduisent le compromis réalisé lors du choix de k. En
optant pour une valeur modérée, nous avons limité la sensibilité du
modèle aux fluctuations locales des données, tout en évitant un
surajustement. Toutefois, l’accuracy relativement faible indique que le
modèle pourrait être amélioré.

### Utisation de la partie Test pour évaluer le classifieur final

Afin d’évaluer la robustesse du modèle, nous avons appliqué notre
classifieur k-NN (k = 4) sur l’ensemble d’entraînement et de validation
regroupés. Cette approche permet de vérifier dans quelle mesure le
modèle s’adapte aux données sur lesquelles il a été appris.

```{r}
set.seed(1237)
target_pred_train = knn(train = rbind(predictive_vars_train, predictive_vars_validation),
                         test = rbind(predictive_vars_train, predictive_vars_validation),
                         cl = c(target_train, target_validation),
                         k = 4)

accuracy_train = mean(target_pred_train == c(target_train, target_validation))
recall_train = recall(data = target_pred_train, c(target_train, target_validation), relevant = "1")
precision_train = precision(target_pred_train, c(target_train, target_validation), relevant = "1")

confusionMatrix(target_pred_train, c(target_train, target_validation))$table
accuracy_train
recall_train
precision_train

```

Les performances obtenues sont les suivantes :

-   Accuracy : 73.23 % : Le modèle classe correctement environ 72 % des
    observations. Cette performance est nettement meilleure que celle
    obtenue sur l’ensemble de test, ce qui suggère un possible
    surajustement du modèle aux données d'entraînement.

-   Recall : 53.12 % : Le modèle parvient à identifier 53.12 % des
    patients réellement à risque d’infarctus. Autrement dit, il détecte
    un peu plus d’un patient à risque sur deux, ce qui montre une
    capacité modérée à repérer correctement les cas positifs.

-   Précision : 64.63 % : Lorsqu’il prédit un élément comme appartenant
    à la classe cible, il a environ 65 % de chances d’avoir raison.
    Cette valeur est plus élevée que celle obtenue sur l’ensemble de
    test, ce qui indique que le modèle fonctionne bien sur les données
    connues mais peut avoir plus de mal à généraliser.

**Analyse des résultats** La différence notable entre les performances
sur l’ensemble d’entraînement et celles obtenues sur l’ensemble de test
indique que le modèle pourrait souffrir d’un légèr surajustement : il
s’adapte bien aux données d’apprentissage mais perd en généralisation
sur de nouvelles données et cela peut être aussi dû probablement à notre
base de données qui presente moins de personnes aynat de risques..

Pour améliorer la performance globale du modèle et éviter cet effet,
plusieurs pistes peuvent être envisagées :

-   Tester d’autres valeurs de k pour identifier un meilleur compromis
    entre recall et précision.

-   Introduire une validation croisée afin d’avoir une évaluation plus
    fiable des performances du modèle.

-   Expérimenter d’autres méthodes de classification comme les arbres de
    décision,naïve bayesienne pour comparer les résultats.

## 1.2 Méthode de cross-validation

Pour pouvoir évaluer la performance du modèle de manière plus robuste et
de déterminer la meilleure valeur de k, nous avons utilisé une
cross-validation en 10 folds. Avant d’effectuer la cross-validation,
nous avons d’abord divisé notre ensemble de données en deux parties
comme dans la méthode hold-out en un ensemble de test (10 % des
observations), qui ne sera utilisé qu’à la toute fin pour évaluer le
modèle. Un ensemble d’apprentissage et de validation (90 % des
observations), qui sera utilisé pour la cross-validation.

La cross-validation est réalisée en 10 itérations (folds). À chaque
itération :

Les données sont divisées en une partie d'entraînement(80%) et une
partie de validation(10%). Le classifieur k-NN est entraîné sur les
données d’entraînement pour chaque valeur de k allant de 1 à 20. Les
performances sont évaluées sur la partie de validation en utilisant
trois métriques :

Accuracy (précision globale)

Recall (sensibilité)

Precision (précision des prédictions positives)

Les performances obtenues pour chaque itération et chaque valeur de k
sont stockées dans des matrices.

```{r, eval = FALSE}
set.seed(1357)
inTest                  = sample(1:nobs, size = round(0.1 * nobs), replace = FALSE)
predictive_vars_test    = predictive_vars[inTest,]
target_test             = target[inTest]
predictive_vars_remain  = predictive_vars[-inTest,] # ce qu'on partagera en Train 
# et Validation à chaque CV itération
target_remain           = target[-inTest]

nobs_remain = nrow(predictive_vars_remain)
nfolds      = 10
KGrid       = 1:20
  
folds   = NULL
for(i in 1:nobs_remain){
  folds = c(folds, sample(1:nfolds, 1, replace = FALSE))
}

accuracy_fold = NULL
recall_fold = NULL
precision_fold = NULL

for(fold in 1: nfolds){ # pour faire les itérations de la CV
  inFold                      = which(folds == fold)
  predictive_vars_train       = predictive_vars_remain[-inFold,]  # pour entrainer le modèle
  predictive_vars_validation  = predictive_vars_remain[inFold,]   # pour évaluer le modèle
  target_train                = target_remain[-inFold]
  target_validation           = target_remain[inFold]
    
 
  accuracy_k  = NULL
  recall_k    = NULL
  precision_k = NULL
  for(k in KGrid){ # pour chaque valeur de K dans KGrid on a fait "l'apprentissage" du 
    # classifieur, puis on prédit la réponse des individidus dans la partie Validation, 
    # et enfin on compare les prédictions aux vraies valeurs de la réponse
    target_pred = knn(train = predictive_vars_train, test = predictive_vars_validation, 
                      cl = target_train, k = k) 
      
    accuracy_k      = c(accuracy_k, mean(target_pred == target_validation))
    recall_k        = c(recall_k, 
                        recall(data = target_pred, target_validation, 
                               relevant = "1"))
    precision_k     = c(precision_k, 
                        precision(target_pred, target_validation, 
                                  relevant = "1"))
    }
    
    accuracy_fold = rbind(accuracy_fold, accuracy_k)
    recall_fold = rbind(recall_fold, recall_k)
    precision_fold = rbind(precision_fold, precision_k)
  }
rownames(accuracy_fold) = paste0("fold_",1:nfolds)
rownames(recall_fold) = paste0("fold_",1:nfolds)
rownames(precision_fold) = paste0("fold_",1:nfolds)
  
colnames(accuracy_fold) = paste0("K_", KGrid)
colnames(recall_fold) = paste0("K_", KGrid)
colnames(precision_fold) = paste0("K_", KGrid)
```

### Calcul des Moyennes sur les 10 Folds

Moyenne de l’accuracy : reflète la proportion de bonnes classifications.
Moyenne du recall : indique la capacité du modèle à détecter la classe
cible. Moyenne de la précision : mesure la fiabilité des prédictions
positives.

```{r}
# 10-fold cross validated mean accuracy pour chaque valeur de K
mean_accuracy_k = colMeans(accuracy_fold)

# 10-fold cross validated mean recall pour chaque valeur de K
mean_recall_k = colMeans(recall_fold)

# 10-fold cross validated mean precision pour chaque valeur de K
mean_precision_k = colMeans(precision_fold)

# Affichage des résultats
mean_accuracy_k
mean_recall_k
mean_precision_k
```

> Analyse des Résultats de la Cross-Validation Après avoir effectué la
> cross-validation, nous avons obtenu les valeurs moyennes des trois
> métriques (accuracy, recall, et precision) pour chaque valeur de k
> allant de 1 à 20.

-   **Accuracy Moyenne par Valeur de K** L’accuracy augmente
    progressivement avec k, atteignant son maximum pour k = 20 avec
    61,10 % de précision. Cela indique que les performances du modèle
    s’améliorent avec un nombre plus élevé de voisins.
-   **Recall Moyen par Valeur de K** Le recall suit une tendance
    croissante, atteignant 90,84 % pour k = 20. Un recall élevé signifie
    que le modèle identifie efficacement les instances positives (classe
    0), ce qui peut être crucial si nous voulons minimiser les faux
    négatifs.
-   **Precision Moyenne par Valeur de K** La précision varie moins que
    les autres métriques et reste relativement stable autour de 63,8 %.
    Cela montre que le modèle ne produit pas trop de faux positifs, mais
    qu'il n’y a pas de grande amélioration avec l’augmentation de k.

Pour notre base, nous allons choisir un k =15 afin que notre modèle
puisse faire une bonne prediction des vrais positifs et dea faux
positifs. Ce choix optimise la performance du classifieur tout en
minimisant les biais liés aux valeurs extrêmes de k.

### Graphes 10-fold mean accuracy en fonction des valeurs de K

```{r, out.width="50%", fig.align = "center"}
# Charger ggplot2 pour les graphiques
library(ggplot2)

# Création d'un data.frame pour ggplot
results = data.frame(
  K = KGrid,
  Accuracy = mean_accuracy_k,
  Recall = mean_recall_k,
  Precision = mean_precision_k
)

# Graphique de l'accuracy
ggplot(results, aes(x = K, y = Accuracy)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  labs(title = "10-Fold Mean Accuracy vs. K", x = "K", y = "Mean Accuracy") +
  theme_minimal()

# Graphique du recall
ggplot(results, aes(x = K, y = Recall)) +
  geom_line(color = "red") +
  geom_point(color = "red") +
  labs(title = "10-Fold Mean Recall vs. K", x = "K", y = "Mean Recall") +
  theme_minimal()

# Graphique de la précision
ggplot(results, aes(x = K, y = Precision)) +
  geom_line(color = "green") +
  geom_point(color = "green") +
  labs(title = "10-Fold Mean Precision vs. K", x = "K", y = "Mean Precision") +
  theme_minimal()


```

### Performance du classifieur final

```{r, eval = FALSE}
library(caret)
library(class)

# Application du modèle kNN avec le meilleur K trouvé (ex: K = 3, à ajuster)
best_k = 20 # Mettre la meilleure valeur de K trouvée
target_pred = knn(train = predictive_vars_remain,
                  test = predictive_vars_test, 
                  cl = target_remain, 
                  k = best_k)

# Calcul des performances sur le jeu de test
accuracy_test  = mean(target_pred == target_test)
recall_test    = recall(data = target_pred, target_test, 
                        relevant = "1")
precision_test = precision(target_pred, target_test, 
                           relevant = "1")

# Affichage des résultats
accuracy_test
recall_test 
precision_test

```

## 2. La régression logistique

La régression logistique est l’un des algorithmes de classification les
plus utilisés lorsqu’on souhaite modéliser une variable binaire.
Contrairement à la méthode des k plus proches voisins (k-NN) qui peut
fonctionner directement avec des variables catégorielles encodées, la
régression logistique nécessite que **toutes les variables explicatives
soient numériques**. Dans ce cadre, nous réutilisons le jeu de données
`predictive_vars` déjà transformé dans la phase de prétraitement. Le
modèle est entraîné sur l’échantillon d’apprentissage
(`predictive_vars_train`) en utilisant la variable cible `target_train`.
La prédiction est ensuite effectuée sur l’échantillon test
(`predictive_vars_test`), et le résultat est exprimé sous forme de
**probabilités** d’appartenir à la classe positive (i.e., présence de
risque cardiaque). Un **seuil de 0.5** est appliqué pour transformer les
probabilités en classes (0 ou 1), et une **matrice de confusion** est
calculée pour évaluer la performance du modèle (accuracy, recall,
précision...).

```{r}

library(caret)
```

```{r}
predictive_vars=cbind(quantitative_vars_mmnormed, qual_dummies)
# Tout d’abord, s’assurer que target est bien factor
target <- factor(target, levels = c("0", "1"))

# Conversion des données pour modèle
X <- model.matrix(~ ., data = predictive_vars)[, -1]  # retire l'intercept automatiquement
X <- as.data.frame(X)  # convertir en data frame si besoin

```

```{r}
set.seed(1237) #pour la reproductibilité 
set.seed(123)
train_index <- createDataPartition(target, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
X_test  <- X[-train_index, ]
target_train <- target[train_index]
target_test  <- target[-train_index]

```

```{r}

logit_model <- glm(target_train ~ ., data = X_train, family = binomial)
summary(logit_model)
target_pred_prob <- predict(logit_model, newdata = X_test, type = "response")
target_pred_class <- ifelse(target_pred_prob > 0.5, 1, 0)
target_pred_class <- factor(target_pred_class, levels = c("0", "1"))

confusionMatrix(target_pred_class, target_test, positive = "1")

```

**Interpretation**

-    Accuracy : 0.6419. Le modèle est correct dans 64% des cas. C’est un
    peu moins bon que le taux de majorité (0.6446), donc pas très
    performant globalement.

-    Sensitivity : 0. Le modèle échoue totalement à détecter les
    patients à risque (classe 1). Aucun des 627 cas réellement à risque
    n’a été correctement identifié.
    Ce score est très préoccupant, surtout dans un contexte médical, car
    il implique un risque élevé de faux négatifs, c’est-à-dire des
    patients à risque laissés sans prise en charge.

-    Specificity : 1. Tous les patients non à risque (classe 0) ont été
    correctement prédits.

-   Precision : NA. Le modèle n’a prédit aucun individu comme étant à
    risque, il est donc impossible de calculer une précision, car le
    dénominateur est nul (aucune prédiction positive).

Dans un contexte médical, ce modèle est inacceptable, car il laisse
passer 100 % des cas à risque, ce qui compromet la sécurité des
patients.

## 3. Arbre de classification

#### Variables explicatives

Dans cette section, nous implémentons un arbre de décision pour prédire
le risque de crise cardiaque. L’algorithme est entraîné sur l’ensemble
d’entraînement (predictive_vars_train) Avant de construire l’arbre de
classification, nous avons regroupé l’ensemble des variables
explicatives — à la fois quantitatives et qualitatives — dans un seul
objet nommé predictive_vars. Une vérification de la structure (str)
permet de s'assurer que les types de données sont correctement
interprétés (par exemple, les variables qualitatives sont bien des
facteurs, les quantitatives sont numériques). Cette étape garantit une
préparation adéquate des données en vue de l'apprentissage du modèle.

```{r}
predictive_vars = cbind(quantitative_vars, qualitative_vars)
head(predictive_vars)
str(predictive_vars)
```

#### Partage du jeu de données

Afin d’évaluer correctement les performances de notre modèle, nous avons
séparé le jeu de données en deux sous-ensembles : 80% des données ont
été utilisées pour l'entraînement du modèle (train), 20% restantes ont
été conservées pour le test final (test).

Le partage s’effectue de manière aléatoire mais reproductible grâce à la
fonction set.seed().

```{r, eval = FALSE}

set.seed(1237)
nobs    = nrow(qualitative_vars) 
inTrain = sample(1:nobs, size = round(0.8 * nobs), replace = FALSE)

predictive_vars_test    = predictive_vars[-inTrain,]
target_test             = target[-inTrain] 
predictive_vars_train   = predictive_vars[inTrain,]
target_train            = target[inTrain] 
```

#### Installation des packages et Chargement des librairies `rpart`et `rpart.plot`

Nous utilisons les bibliothèques rpart et rpart.plot pour la création et
la visualisation de l’arbre de décision.

```{r, results = "hide", message = FALSE}
install.packages("rpart")
install.packages("rpart.plot")
```

```{r}
library(rpart)
library(rpart.plot)
```

### 3.1. Préparation des données

Nous convertissons la variable cible en facteur, car rpart attend une
variable catégorielle pour la classification. De plus, les ensembles de
données d'entraînement et de test sont transformés en data frames pour
assurer leur compatibilité avec l'algorithme.

```{r, eval = FALSE}
predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_test <- as.data.frame(predictive_vars_test)
target_train <- as.factor(target_train)

```

### 3.3 Phase d'apprentissage

L’arbre est entraîné avec une profondeur maximale de 5 niveaux, un
critère de division minimal de 10 observations par nœud, et un paramètre
de complexité (cp) de 0.001 pour éviter un sur-ajustement excessif.

```{r, eval = FALSE}
classifTree <- rpart(target_train ~ ., data = predictive_vars_train, 
                     method = "class",
                     control = rpart.control(minsplit = 10, cp = 0.001, maxdepth = 5))

```

####  Visualisation de l’arbre de décision

Une fois l’arbre entraîné, nous utilisons rpart.plot pour afficher sa
structure, en mettant en évidence les probabilités et les règles de
classification.

```{r, eval = FALSE}
rpart.plot(classifTree, type = 3, extra = 2, fallen.leaves = TRUE,  
           main = "Arbre de prédiction du risque de crise cardiaque") 
```

### 3.4 Prédiction de la target avec l'arbre de décision sur le jeu de données de test

### Visualisation de l'arbre en phase de test

```{r, eval = FALSE}
classifTree1 <- rpart(target_test ~ ., data = predictive_vars_test, 
                     method = "class",
                     control = rpart.control(minsplit = 10, cp = 0.001, maxdepth = 5))
rpart.plot(classifTree1, type = 3, extra = 2, fallen.leaves = TRUE,  
           main = "Arbre de prédiction du risque de crise cardiaque") 

```

Nous évaluons la performance de notre arbre de décision sur l’ensemble
de test en calculant les métriques de classification en utilisant une
matrice de confusion (confusionMatrix)

```{r, eval = FALSE}
# Prédiction sur le jeu de test
set.seed(1234)
target_pred = predict(classifTree, predictive_vars_test, type = "class")

# Calcul des métriques
cm = confusionMatrix(target_pred, target_test, positive = "1")

# Affichage des résultats
accuracy_test  = cm$overall["Accuracy"]
recall_test    = cm$byClass["Sensitivity"]  # Sensitivity = Recall
precision_test = cm$byClass["Precision"]

accuracy_test
recall_test
precision_test
```

**interprétation des résultats** Les performances de l’arbre de décision
sur le jeu de test sont les suivantes :

-   Accuracy : 62,86 %, Cela signifie que le modèle effectue
    correctement environ 62 % des prédictions sur l’ensemble de test.
-   Sensitivity (Recall) : 16.2 %, Ce faible score indique que le modèle
    ne détecte qu’une petite partie des cas positifs (patients à
    risque). Autrement dit, il y a un grand nombre de faux négatifs.
-   Precision : 57.29 %, Cela signifie qu’environ 57% des prédictions
    positives sont correctes.

####  Élagage de l'arbre

```{r, eval = FALSE}
classifTree$cptable
```

```{r, eval = FALSE}
cp = classifTree$cptable[which.min(classifTree$cptable[, "xerror"]), "CP"]
cp

printcp(classifTree)  # Affiche les valeurs de cp
plotcp(classifTree)   # Graphique de l'erreur en fonction de cp

```

#### Visualisation de l’arbre élagué

L’arbre de décision obtenu après élagage est visualisé ci-dessous. Cette
représentation graphique permet d’identifier les variables les plus
importantes dans le processus de classification, ainsi que les règles
utilisées pour distinguer les individus à risque d’infarctus.

Elle constitue également un outil d’interprétation utile dans un
contexte médical, car elle permet de suivre la logique de décision étape
par étape.

```{r, eval = FALSE}
prunedTree = prune(classifTree, cp*0.8)

# Tracer l’arbre élagué
rpart.plot(prunedTree, type = 3, extra = 2, fallen.leaves = TRUE,  
           main = "Arbre élagué de prédiction du risque de crise cardiaque") 

```

#### Remarque sur l’arbre élagué

> Il est important de souligner que l’arbre utilisé pour prédire la
> variable cible sur le jeu de test correspond exactement à l’arbre
> élagué obtenu lors de l’apprentissage. Ce comportement est **normal et
> conforme à la méthodologie en apprentissage supervisé** : le modèle
> est entraîné exclusivement sur l’échantillon d’apprentissage, puis
> utilisé tel quel pour effectuer des prédictions sur de nouvelles
> observations.

> Ainsi, bien que l’on parle d’"arbre de test", il ne s’agit pas d’un
> nouvel arbre construit à partir des données de test, mais bien du
> **même arbre** que celui obtenu après élagage, appliqué cette fois à
> un **jeu de données indépendant**. Ce procédé garantit une évaluation
> fidèle de la capacité de généralisation du modèle, sans biais
> d’apprentissage.

> La structure de l’arbre (les variables utilisées, les règles de
> séparation, la profondeur) reste donc **inchangée** entre
> l'entraînement et le test. Seuls les résultats prédictifs diffèrent.

Nous utilisons l’arbre élagué pour effectuer des prédictions sur le jeu
de test

```{r}
 # Prédiction avec l'arbre élagué
target_pred_pruned = predict(prunedTree, predictive_vars_train, type = "class")

# Calcul des métriques
cm_pruned = confusionMatrix(target_pred_pruned, target_train, positive = "1")

accuracy_train_pruned  = cm_pruned$overall["Accuracy"]
recall_train_pruned    = cm_pruned$byClass["Sensitivity"]
precision_train_pruned = cm_pruned$byClass["Precision"]

accuracy_train_pruned
recall_train_pruned
precision_train_pruned

```

### Interprétation des performances sur le jeu d'entraînement

Le modèle d’arbre de décision élagué a été évalué sur l’ensemble
d’apprentissage. Les résultats obtenus sont les suivants :

-   **Accuracy** : 63.02 %. Le modèle réalise correctement 63 % des
    prédictions globales, c’est-à-dire qu’il identifie correctement
    environ 2 individus sur 3, toutes classes confondues.

-   **Recall (Sensibilité)** : 8.98 %. Cela signifie que plus de 91 %
    des patients réellement à risque ne sont pas identifiés par le
    modèle. Ce niveau de sensibilité est très insuffisant dans un cadre
    médical, où la priorité est de limiter les faux négatifs pour éviter
    que des patients à risque soient ignorés.

-   **Précision** : 38.50 %. Lorsqu’un patient est identifié comme étant
    à risque, il y a seulement 38.5 % de chances qu’il le soit
    réellement.

> Ces résultats indiquent que bien que le modèle obtienne une précision
> globale (accuracy) modérée, il présente une très faible sensibilité,
> c’est-à-dire qu’il parvient difficilement à identifier correctement
> les individus réellement à risque (classe 1). En d’autres termes, le
> modèle a tendance à prédire principalement la classe majoritaire
> (absence de risque), au détriment de la classe minoritaire (présence
> de risque).

La précision relativement faible (38.5 %) signifie par ailleurs que
lorsque le modèle prédit un individu à risque, il se trompe plus d'une
fois sur deux. Cela illustre un déséquilibre dans la performance du
modèle entre les deux classes, qui pourrait être lié : - à un
déséquilibre de classes dans les données d’entraînement, - à un
sur-ajustement (overfitting) avant l’élagage, - ou encore à une
complexité insuffisante du modèle après élagage.

Ces éléments justifient l’importance d’une analyse sur le **jeu de
test** et d’une comparaison avec d’autres approches (régression
logistique, k-NN, Naïf Bayes...) pour évaluer la robustesse du
classifieur.

### 3.5 Prédiction de la target avec l'arbre élagué sur le jeu de données de test

```{r}
 # Prédiction avec l'arbre élagué
target_pred_pruned = predict(prunedTree, predictive_vars_test, type = "class")

# Calcul des métriques
cm_pruned = confusionMatrix(target_pred_pruned, target_test, positive = "1")

accuracy_test_pruned  = cm_pruned$overall["Accuracy"]
recall_test_pruned    = cm_pruned$byClass["Sensitivity"]
precision_test_pruned = cm_pruned$byClass["Precision"]

accuracy_test_pruned
recall_test_pruned
precision_test_pruned
```

### 3.6 Interprétation des performances sur le jeu de test

L’arbre élagué a ensuite été appliqué au jeu de données de test, afin
d’évaluer sa capacité de généralisation. Les métriques obtenues sont les
suivantes :

-   **Accuracy** : 62.86 %\
-   **Recall (Sensibilité)** : 16.20 %\
-   **Précision** : 57.92 %

> Ces résultats indiquent une **performance globale modérée**, avec une
> accuracy relativement stable par rapport au jeu d’apprentissage.
> Toutefois, on observe que la **sensibilité reste faible** : le modèle
> ne parvient à détecter qu’environ **16 % des patients réellement à
> risque**, ce qui est problématique dans un contexte médical où **les
> faux négatifs doivent être minimisés**.

La **précision**, en revanche, est plus élevée : parmi les patients
prédits comme étant à risque, **environ 58 % le sont effectivement**.
Cela signifie que, bien que le modèle **prédise peu de cas positifs**,
il est **plutôt fiable lorsqu’il en identifie un**.

Ce compromis entre **faible sensibilité et précision raisonnable** peut
s’expliquer par : - la dominance de la classe "0" (absence de risque)
dans les données, - le **caractère conservateur de l’arbre élagué**, qui
évite les prédictions positives hasardeuses, - ou encore une **structure
de données complexe**, difficile à séparer avec des règles arborescentes
simples.

> Dans l’ensemble, bien que ce modèle offre une certaine **robustesse
> sur le test**, il manque de **puissance pour identifier efficacement
> les individus à risque**, ce qui limite son utilité en situation
> clinique sans ajustement ou méthode complémentaire.

### 3.7 Validation croisée (10-fold) – Arbre sans élagage

Afin d’évaluer la robustesse du modèle d’arbre de décision sans élagage,
une **validation croisée à 10 plis** a été réalisée. Cette méthode
consiste à diviser le jeu de données en 10 sous-ensembles, à entraîner
le modèle sur 9 d’entre eux, et à l’évaluer sur le dixième, de manière
itérative.

```{r}
# Charger les packages nécessaires
library(caret)
library(rpart)
library(rpart.plot)
```

```{r}

set.seed(1357)

# Définition des folds
nobs                  <- nrow(predictive_vars)
inTest                <- sample(1:nobs, size = round(0.1 * nobs), replace = FALSE)
predictive_vars_test  <- predictive_vars[inTest,]
target_test           <- target[inTest]
predictive_vars_remain <- predictive_vars[-inTest,]
target_remain         <- target[-inTest]

nobs_remain <- nrow(predictive_vars_remain)
nfolds      <- 10

# Création des folds manuellement
folds <- sample(rep(1:nfolds, length.out = nobs_remain))

# Matrices pour stocker les métriques
accuracy_fold <- NULL
recall_fold <- NULL
precision_fold <- NULL

# Boucle sur les folds
for (fold in 1:nfolds) {
  # Séparation des données en train et validation
  inFold                      <- which(folds == fold)
  predictive_vars_train       <- predictive_vars_remain[-inFold, ]
  predictive_vars_validation  <- predictive_vars_remain[inFold, ]
  target_train                <- target_remain[-inFold]
  target_validation           <- target_remain[inFold]
    
  accuracy_k  <- NULL
  recall_k    <- NULL
  precision_k <- NULL

  # Entraînement de l'arbre de décision sans élagage
  tree_model <- rpart(target_train ~ ., data = predictive_vars_train, method = "class")
  
  # Prédiction sur la partie validation
  target_pred <- predict(tree_model, predictive_vars_validation, type = "class")

  # Calcul des métriques avec confusionMatrix()
  cm <- confusionMatrix(target_pred, target_validation, positive = "1")
  
  # Stocker les résultats
  accuracy_k <- cm$overall["Accuracy"]
  recall_k   <- cm$byClass["Sensitivity"]
  precision_k <- cm$byClass["Precision"]

  accuracy_fold <- rbind(accuracy_fold, accuracy_k)
  recall_fold   <- rbind(recall_fold, recall_k)
  precision_fold <- rbind(precision_fold, precision_k)
}

rownames(accuracy_fold) <- paste0("fold_", 1:nfolds)
rownames(recall_fold)   <- paste0("fold_", 1:nfolds)
rownames(precision_fold) <- paste0("fold_", 1:nfolds)

```

Calcul de la 10-fold cross validated mean accuracy, la 10-fold cross
validated mean recall, et la 10-fold cross validated mean precision

```{r}
# Calcul des moyennes
mean_accuracy  <- mean(accuracy_fold)
mean_recall    <- mean(recall_fold)
mean_precision <- mean(precision_fold)

# Affichage des résultats
cat("10-fold Cross-Validation - Arbre sans élagage :\n")
cat("Mean Accuracy :", mean_accuracy, "\n")
cat("Mean Recall :", mean_recall, "\n")
cat("Mean Precision :", mean_precision, "\n")
```

Les résultats moyens obtenus sont les suivants :

-   **Accuracy moyenne** : 64.27 %\
-   **Recall moyen** : 0.00 %\
-   **Précision moyenne** : indéfinie (`NA`)

> Ces résultats révèlent une **limite majeure du modèle** : il ne
> parvient **jamais à prédire la classe positive (à risque)** sur
> l’ensemble des folds. Par conséquent : - Le **recall** est nul car
> aucun individu réellement à risque n’est correctement identifié. - La
> **précision** est non définie (`NA`) car le modèle ne produit **aucune
> prédiction positive** (le dénominateur est nul dans le calcul de la
> précision).

Ce phénomène est souvent observé lorsque : - Le modèle est **trop biaisé
vers la classe majoritaire**, - Le **seuil de décision implicite** (0.5)
est trop conservateur, - Les données sont **déséquilibrées** et
**l’arbre est trop profond**, ce qui renforce la surreprésentation de la
classe majoritaire.

Ces observations confirment que, **sans élagage ni ajustement**, l’arbre
de décision a **une capacité de généralisation très faible pour détecter
les individus à risque**. Cela justifie pleinement l’utilisation : - de
l’élagage, - d’un ajustement du seuil de prédiction, - voire de
**méthodes de rééquilibrage** (SMOTE, suréchantillonnage) pour améliorer
les performances sur la classe minoritaire.

### 3.8 Validation croisée (10-fold) – Arbre avec élagage

```{r}
# Matrices pour stocker les métriques après élagage
accuracy_fold_pruned <- NULL
recall_fold_pruned <- NULL
precision_fold_pruned <- NULL

# Boucle sur les folds avec élagage
for (fold in 1:nfolds) {
  # Séparation des données en train et validation
  inFold                      <- which(folds == fold)
  predictive_vars_train       <- predictive_vars_remain[-inFold, ]
  predictive_vars_validation  <- predictive_vars_remain[inFold, ]
  target_train                <- target_remain[-inFold]
  target_validation           <- target_remain[inFold]
    
  accuracy_k  <- NULL
  recall_k    <- NULL
  precision_k <- NULL

  # Entraînement de l'arbre de décision sans élagage
  tree_model <- rpart(target_train ~ ., data = predictive_vars_train, method = "class")

  # Sélection du meilleur cp pour l'élagage
  best_cp <- tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"]
  
  # Élagage de l'arbre
  pruned_tree_model <- prune(tree_model, cp = best_cp)

  # Prédiction sur la partie validation
  target_pred <- predict(pruned_tree_model, predictive_vars_validation, type = "class")

  # Calcul des métriques avec confusionMatrix()
  cm <- confusionMatrix(target_pred, target_validation, positive = "1")
  
  # Stocker les résultats
  accuracy_k <- cm$overall["Accuracy"]
  recall_k   <- cm$byClass["Sensitivity"]
  precision_k <- cm$byClass["Precision"]

  accuracy_fold_pruned <- rbind(accuracy_fold_pruned, accuracy_k)
  recall_fold_pruned   <- rbind(recall_fold_pruned, recall_k)
  precision_fold_pruned <- rbind(precision_fold_pruned, precision_k)
}

rownames(accuracy_fold_pruned) <- paste0("fold_", 1:nfolds)
rownames(recall_fold_pruned)   <- paste0("fold_", 1:nfolds)
rownames(precision_fold_pruned) <- paste0("fold_", 1:nfolds)

```

Calculer la 10-fold cross validated mean accuracy, la 10-fold cross
validated mean recall, et la 10-fold cross validated mean precision

```{r}
# Calcul des moyennes
mean_accuracy_pruned  <- mean(accuracy_fold_pruned)
mean_recall_pruned    <- mean(recall_fold_pruned)
mean_precision_pruned <- mean(precision_fold_pruned)

# Affichage des résultats
cat("10-fold Cross-Validation - Arbre avec élagage :\n")
cat("Mean Accuracy :", mean_accuracy_pruned, "\n")
cat("Mean Recall :", mean_recall_pruned, "\n")
cat("Mean Precision :", mean_precision_pruned, "\n")

```

### 3.9 Interprétation des résultats de la cross-validation (avec élagage)

Une validation croisée à 10 plis a été réalisée en intégrant l'élagage à
chaque itération. L’objectif était d’évaluer la performance moyenne d’un
arbre élagué, optimisé à l’aide du paramètre de complexité (`cp`) issu
de la minimisation de l’erreur de validation croisée (`xerror`).

Les résultats moyens obtenus sont les suivants :

-   **Accuracy moyenne** : 64.28 %\
-   **Recall moyen** : 0.00 %\
-   **Précision moyenne** : NA

> Bien que l’accuracy reste correcte, ces résultats soulignent une
> **grande faiblesse du modèle dans la détection des patients à risque
> (classe 1)**. En effet, le modèle ne prédit **jamais la classe
> positive**, ce qui entraîne un **rappel nul** et une **précision non
> définie** (aucune prédiction positive).

Ce comportement est typique des situations de **déséquilibre des
classes**, dans lesquelles le modèle favorise la classe dominante (ici,
l'absence de risque) pour maximiser sa performance globale. L'élagage,
en simplifiant la structure de l’arbre, **réduit encore plus sa
sensibilité** aux cas minoritaires.

> Cette situation justifie pleinement le recours à des techniques de
> **rééquilibrage des classes** (par exemple : sur-échantillonnage,
> sous-échantillonnage, ou la méthode SMOTE), ainsi qu’à des **modèles
> plus robustes** pour traiter des problématiques médicales où
> **minimiser les faux négatifs est crucial**.

## 4. Classification Naïve Bayésienne

### 4.1 Apprentissage du classifieur

La méthode de classification **Naïve Bayésienne** repose sur le
**théorème de Bayes**, en faisant l’hypothèse d’indépendance
conditionnelle entre les variables explicatives. Bien que cette
hypothèse soit souvent irréaliste, ce classifieur reste très utilisé
pour sa **simplicité**, sa **vitesse d’apprentissage** et sa
**robustesse face aux petits jeux de données**.

Le modèle a été entraîné à l’aide du package `naivebayes` sur un jeu de
données préalablement divisé en deux ensembles :

\- **80 % pour l'entraînement** (`predictive_vars_train`,
`target_train`)

\- **20 % pour le test** (`predictive_vars_test`, `target_test`)

Le tableau de sortie du classifieur contient :

\- Les **probabilités a priori** (proportions de chaque classe dans
l’échantillon d’apprentissage)

\- Les **statistiques conditionnelles** (moyennes et variances des
variables continues selon la classe cible).

```{r}
predictive_vars = cbind(quantitative_vars, qualitative_vars)
```

```{r}
set.seed(1234)
nobs    = nrow(qualitative_vars) 
inTrain = sample(1:nobs, size = round(0.8 * nobs), replace = FALSE)

predictive_vars_test    = predictive_vars[-inTrain,]
target_test             = target[-inTrain] 
predictive_vars_train   = predictive_vars[inTrain,]
target_train            = target[inTrain] 
target_train <- as.factor(target_train)
predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_test <- as.data.frame(predictive_vars_test)
```

```{r, results = "hide", message = FALSE}
library(naivebayes)
?naive_bayes
```

```{r}
classif = naive_bayes(x = predictive_vars_train, y = target_train)
classif
```

### 4.2 Interprétation des résultats

Les probabilités a priori calculées par le modèle sont :

-   Classe 0 (patients non à risque) : **64 %**
-   Classe 1 (patients à risque) : **36 %**

Cela reflète la répartition observée dans le jeu d'entraînement, sans
déséquilibre majeur.

Voici quelques observations sur certaines variables conditionnelles :

-   **Cholestérol** : La moyenne est légèrement plus élevée chez les
    patients à risque (`0.506` contre `0.497`), ce qui est cohérent avec
    les connaissances médicales.\
    Cela renforce l’idée que le cholestérol est un facteur contributif
    dans la prédiction du risque cardiaque.

-   **Heart.Rate** : Les moyennes entre les deux classes sont quasiment
    identiques (`0.500` vs `0.495`), suggérant un **faible pouvoir
    discriminant** de cette variable dans le cadre de ce modèle.

-   **BMI (indice de masse corporelle)** : La moyenne est très similaire
    entre les classes (`0.498` vs `0.497`).\
    Cette absence de différence statistique marquée limite l’utilité de
    cette variable pour la classification selon l’approche bayésienne.

-   **Triglycérides** : La moyenne est très similaire entre les classes
    (`418.76` vs `418.97`).\
    Les moyennes des triglycérides sont quasiment identiques dans les
    deux classes. Cela suggère que cette variable **n’a pas de pouvoir
    discriminant significatif** dans le modèle naïf bayésien. Son impact
    sur la prédiction du risque d’infarctus est donc très limité dans ce
    cadre.

-   **Pression artérielle systolique (Systolic_BP)** : La moyenne est
    très similaire entre les classes (`134.94` vs `135.63`). Là encore,
    les moyennes sont très proches, avec une légère élévation pour la
    classe à risque. Toutefois, l’écart est **trop faible pour
    constituer un facteur discriminant fiable** pour ce modèle. Cette
    observation met en évidence une limite du modèle naïf bayésien
    lorsque les distributions des variables sont **fortement
    superposées** entre les classes.

> Globalement, ces résultats confirment que certaines variables, bien
> que médicalement pertinentes, **n’apportent pas d’information
> significative** dans la classification si leurs distributions sont
> trop similaires entre les groupes.

### 4.3 Visualisation des densités conditionnelles

Une des forces du modèle naïf bayésien est sa capacité à représenter,
pour chaque variable continue, la **densité conditionnelle** selon la
classe cible. Cela permet de **visualiser le pouvoir discriminant** de
chaque variable dans la séparation entre les individus à risque et ceux
qui ne le sont pas.

Le graphique suivant présente les courbes de densité estimées pour les
différentes variables explicatives, conditionnellement à la classe (0 =
non à risque, 1 = à risque).

```{r, out.width="50%", fig.align = "center", eval = FALSE}
plot(classif, prob = "conditional")
```

**Interprétation de quelques variables**

-   Income : Les courbes sont quasi identiques pour les deux classes, ce
    qui suggère que cette variable n’est pas discriminante dans le cadre
    du modèle naïf bayésien. Il en va de même pour BMI et
    Sedentary.Hours.Per.Day.

-   Cholesterol : Les courbes présentent un décalage significatif entre
    les classes. Cela laisse penser que le cholestérol est un facteur
    potentiellement informatif dans la classification.

-   Systolic_BP et Exercise.Hours.Per.Week : présentent également des
    courbes différenciées entre les deux groupes, indiquant un pouvoir
    prédictif modéré à bon.

-   Triglycerides : Les densités sont quasiment superposées, ce qui
    traduit une faible capacité à différencier les classes sur cette
    variable.

-   Medication.Use (Utilisation de médicaments) : On observe une
    répartition très proche pour les deux classes (0 et 1). Les
    proportions des individus **utilisant des médicaments (1)** sont
    **similaires** dans les deux groupes (à risque et non à risque), de
    même pour les individus **n'utilisant pas de médicaments (0)**.

-   Smoking (Tabagisme) :

On observe que la distribution des individus selon le statut tabagique
est **très similaire** dans les deux classes de la variable cible. La
proportion de **fumeurs (1)** est quasiment la même chez les patients à
risque et ceux qui ne le sont pas. Il en va de même pour les
**non-fumeurs (0)**.

> Cette homogénéité des proportions indique que **le tabagisme n’a pas,
> dans ce jeu de données, un rôle discriminant majeur** pour la
> classification entre patients à risque et non à risque, du point de
> vue du modèle naïf bayésien. Dans tous les cas, cette observation
> confirme que le modèle naïf bayésien **est sensible à la séparation
> claire des classes dans chaque variable**, ce qui n’est pas le cas ici
> pour `Smoking`.

### 4.4 Prédiction finale et évaluation des performances

Après l'entraînement du classifieur naïf bayésien, nous appliquons ce
modèle à l’échantillon de test afin de prédire la catégorie (à risque ou
non) de chaque individu. Cette étape permet de mesurer la **capacité de
généralisation** du modèle sur des données nouvelles.

La performance est évaluée à l’aide des trois métriques suivantes :

-   **Accuracy** : proportion de bonnes prédictions sur l’ensemble du
    test.
-   **Recall (sensibilité)** : capacité du modèle à détecter
    correctement les individus réellement à risque.
-   **Précision** : proportion de patients prédits comme à risque qui le
    sont réellement.

L’ensemble de ces indicateurs permet d’avoir une vision complète de la
qualité du modèle, en particulier dans un contexte médical où les faux
négatifs peuvent avoir des conséquences graves.

```{r, eval = FALSE}
pred = predict(classif, predictive_vars_test, type = "prob")
```

```{r}
# Prédiction des classes sur l'ensemble de test
target_pred <- predict(classif, predictive_vars_test, type = "class")

# Matrice de confusion avec caret
library(caret)
conf_matrix_nb <- confusionMatrix(target_pred, target_test, positive = "1")
conf_matrix_nb

# Calcul des métriques principales
accuracy <- conf_matrix_nb$overall["Accuracy"]
recall_value <- conf_matrix_nb$byClass["Sensitivity"]   # Sensibilité = recall
precision_value <- conf_matrix_nb$byClass["Precision"]

# Affichage
accuracy
recall_value
precision_value
```

### 4.5 Évaluation du modèle Naïve Bayes sur le jeu de test

Une fois entraîné, le classifieur Naïve Bayes a été appliqué au jeu de
test afin d’évaluer ses performances. La matrice de confusion et les
indicateurs clés sont présentés ci-dessous :

-   **Accuracy** : 64.46%
-   **Recall (sensibilité)** : 0.00%
-   **Précision** : NA
-   **Spécificité** : 100%

> Ces résultats indiquent que le modèle **ne détecte aucun individu
> réellement à risque (classe 1)**. Cela se traduit par un **rappel
> nul** et une **précision non définie**, car aucune prédiction n’a été
> faite pour la classe positive.

> Cette situation, bien que conduisant à une accuracy apparemment
> correcte, révèle un **grave déséquilibre** dans les performances du
> modèle, qui privilégie fortement la classe majoritaire. Le **Kappa
> nul** confirme l'inefficacité du modèle à apporter une valeur
> prédictive réelle au-delà du simple hasard.

> En contexte médical, ce type de modèle serait **inutilisable en
> pratique**, car il **échoue à identifier les cas à risque**, ce qui va
> à l’encontre de l’objectif principal du projet.

Ces résultats mettent en évidence l’importance :

\- de **rééquilibrer les classes** (via suréchantillonnage, SMOTE,
etc.),

\- ou d'utiliser des **modèles plus flexibles** capables de gérer les
déséquilibres de manière plus efficace (comme les forêts aléatoires,
XGBoost ou des modèles pénalisés).

### 4.6 Importance des variables explicatives dans la prédiction

Dans cette section, nous explorons l’influence de certaines variables
explicatives sur les **probabilités prédictives de risque d’infarctus**
estimées par le modèle. Cette approche permet de mieux comprendre
comment les caractéristiques des patients influencent la décision du
classifieur, en particulier dans un contexte médical où
l’interprétabilité est essentielle.

Le graphique suivant présente un **boxplot des probabilités prédites
d’être à risque d’infarctus** (classe `1`), selon le sexe du patient
(`Sex`). Chaque boîte représente la distribution des scores de risque
prédits pour un sous-groupe (femmes vs hommes).

```{r, out.width="50%", fig.align = "center", eval = FALSE}
# Liste des variables catégorielles à tester
vars <- c("Sex", "Previous.Heart.Problems", "Age_cat")

# Configuration pour afficher 2 lignes, 2 colonnes de graphiques
par(mfrow = c(2, 2))

# Boucle pour tracer les boxplots
for (v in vars) {
  boxplot(pred[,2] ~ predictive_vars_test[[v]],
          col = c("lightgreen", "lightblue", "lightyellow", "lightpink"),
          main = paste("Effet de", v),
          ylab = "Probabilité prédite de présence de risque",
          xlab = v)
}

# Cas particulier : variable continue (BMI → scatter plot)
plot(predictive_vars_test$BMI, pred[,2],
     pch = 20, col = "darkgray",
     main = "Relation entre BMI et probabilité de risque",
     ylab = "Probabilité prédite", xlab = "BMI")


```

Afin de mieux comprendre comment certaines variables influencent les
prédictions du modèle (probabilité d’être à risque d’infarctus),
plusieurs visualisations ont été réalisées. Elles permettent de comparer
les **probabilités prédictives** de la classe `1` selon différentes
variables explicatives.

#### 🔹 Sexe (`Sex`)

Le boxplot des probabilités prédites selon le sexe montre que les hommes
ont une **médiane de risque légèrement plus élevée** que les femmes.
Toutefois, la dispersion des valeurs reste globalement similaire entre
les deux groupes, avec une distribution relativement homogène.\
\> Cela indique que le sexe **influence modérément la probabilité
prédite**, sans être une variable discriminante forte à elle seule. Il
peut cependant contribuer au risque lorsqu’il est combiné à d’autres
facteurs.

#### 🔹 Antécédents cardiaques (`Previous.Heart.Problems`)

La comparaison des probabilités entre les patients avec (`1`) ou sans
(`0`) antécédents de problèmes cardiaques révèle **des médianes très
proches**.\
\> Fait surprenant, les individus ayant déjà eu des problèmes cardiaques
ne présentent pas des probabilités sensiblement plus élevées. Cela peut
être dû au fait que cette variable est **corrélée à d’autres variables**
ou que le modèle naïf bayésien **n’exploite pas efficacement les
interactions entre facteurs**.

#### 🔹 Tranches d’âge (`Age_cat`)

Le boxplot des tranches d’âge (`Jeune`, `Middle-aged`, `Senior`,
`Elderly`) révèle une **tendance modérée mais non linéaire**. Les
groupes Middle-aged et Elderly présentent une **légère élévation des
probabilités prédictives** par rapport aux groupes Jeune et Senior.\
\> L'effet de l'âge semble donc présent mais non dominant, probablement
**atténué par l’absence de modélisation d’interactions** dans le
classifieur utilisé.

#### 🔹 Indice de masse corporelle (`BMI`)

La visualisation des probabilités en fonction du BMI montre une
**dispersion uniforme**, sans tendance apparente. Les probabilités
prédictives sont concentrées entre 0.33 et 0.39 pour toutes les valeurs
de BMI.\
Ce résultat suggère que, bien que le BMI soit un facteur de risque
reconnu sur le plan médical, il n’apporte **que peu d’information
discriminante** dans le cadre du modèle naïf bayésien utilisé ici.

Dans l’ensemble, ces analyses soulignent que certaines variables (comme
le sexe ou l’âge) ont un **effet modéré sur la prédiction du risque**,
tandis que d'autres, pourtant cliniquement pertinentes (BMI,
antécédents), **n’influencent que faiblement les probabilités
prédictives**. Cela met en évidence la **nécessité d’utiliser des
modèles plus flexibles** et de considérer des interactions pour
améliorer la détection du risque dans ce type de problématique médicale.
