---
title: "Projet"
author: "ruth"
date: "2025-03-07"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction

## Contexte

L’augmentation du nombre de cas de maladies cardiovasculaires à
l’échelle mondiale représente un défi majeur pour les systèmes de santé.
Parmi celles-ci, l’infarctus du myocarde figure comme l'une des
principales causes de mortalité. La prévention, via une détection
précoce des risques, constitue une voie cruciale d’intervention. Dans ce
contexte, l’analyse de données cliniques par des méthodes de
classification supervisée peut permettre de prédire efficacement le
risque d’infarctus chez un patient, et ainsi d'orienter plus rapidement
les stratégies de prise en charge.

## Objectif

Le présent projet vise à développer un modèle prédictif permettant
d’estimer, à partir de données médicales et comportementales, la
probabilité qu’un individu présente un risque d’infarctus. L’objectif
est double : D’une part, explorer les variables influentes dans la
prédiction du risque cardiovasculaire. D’autre part, évaluer les
performances de modèles de classification supervisée (arbre de décision,
k-NN, classification naive bayésienne et la régression logistique) implémentés dans le langage R.

# I. Préparation des données

## 1. Présentation du jeu de données

Le jeu de données utilisé comprend 8763 individus et 26 variables, dont
une variable cible Heart.Attack.Risk indiquant si l’individu présente
(1) ou non (0) un risque d’infarctus. Les observations portent sur des
aspects cliniques (cholestérol, pression artérielle, triglycérides,
etc.), comportementaux (tabagisme, activité physique, régime
alimentaire) et démographiques (âge, sexe, revenu, pays de résidence).

```{r}
data=read.csv("~/Téléchargements/heart_attack_prediction_dataset.csv")
dim(data)
colnames(data)
```

Un aperçu des variables montre une diversité de types (quantitatif
continu, qualitatif binaire ou nominal, ordinal...), ce qui implique un
traitement différencié lors du prétraitement.

## 2. Description et pertinence des variables

Le jeu de données comprend 26 variables. Celles-ci couvrent des
informations cliniques, comportementales, démographiques et
géographiques. Afin de garantir la qualité de la modélisation, il est
essentiel d’identifier les variables les plus pertinentes, et d’en
exclure certaines qui peuvent introduire du bruit ou de la redondance.

Le tableau ci-dessous présente une description synthétique de chaque
variable, son type, ainsi que son niveau de pertinence pour le projet de
prédiction du risque d’infarctus :

```{r message=FALSE, warning=FALSE}
library(knitr)

variable_info <- data.frame(
  Variable = c("Patient.ID", "Age", "Sex", "Cholesterol", "Blood.Pressure", "Heart.Rate",
               "Diabetes", "Family.History", "Smoking", "Obesity", "Alcohol.Consumption",
               "Exercise.Hours.Per.Week", "Diet", "Previous.Heart.Problems",
               "Medication.Use", "Stress.Level", "Sedentary.Hours.Per.Day", "Income",
               "BMI", "Triglycerides", "Physical.Activity.Days.Per.Week",
               "Sleep.Hours.Per.Day", "Country", "Continent", "Hemisphere", "Heart.Attack.Risk"),
  
  Description = c("Identifiant unique du patient", "Âge du patient", "Sexe (Male/Female)",
                  "Taux de cholestérol total", "Tension artérielle (ex: 120/80)",
                  "Fréquence cardiaque", "Présence de diabète", 
                  "Antécédents familiaux de maladies cardiaques",
                  "Fumeur ou non", "Obésité", "Consommation d’alcool",
                  "Heures d’exercice par semaine", "Qualité de l’alimentation",
                  "Antécédents cardiaques", "Utilisation de médicaments", 
                  "Niveau de stress", "Heures assises par jour",
                  "Revenu annuel", "Indice de masse corporelle",
                  "Taux de triglycérides", "Jours d’activité physique/semaine",
                  "Heures de sommeil par jour", "Pays de résidence", 
                  "Continent de résidence", "Hémisphère", 
                  "Risque d’infarctus (0 ou 1)"),
  
  Type = c("Identifiant", "Quantitative", "Qualitative", "Quantitative", "Texte",
           "Quantitative", "Binaire", "Binaire", "Binaire", "Binaire", "Binaire",
           "Quantitative", "Qualitative", "Binaire", "Binaire", "Quantitative",
           "Quantitative", "Quantitative", "Quantitative", "Quantitative",
           "Quantitative", "Quantitative", "Qualitative", "Qualitative",
           "Qualitative", "Binaire"),
  
  Pertinence = c("Non pertinente", "Très pertinente", "Pertinente", "Pertinente",
                 " À transformer", " Pertinente", " Très pertinente",
                 " Pertinente", " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Très pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Non pertinente", " Non pertinente", " Non pertinente",
                 " Variable cible")
)

kable(variable_info, caption = "Tableau 1 : Description des variables et évaluation de leur pertinence", align = "l", booktabs = TRUE)

```

> Certaines variables comme `Patient.ID`, `Country`, `Continent` et
> `Hemisphere` sont écartées, car elles n’apportent pas de valeur
> informative dans la prédiction du risque cardiaque.

## 3. Typologie des variables

Les variables présentes dans le jeu de données peuvent être regroupées
en deux grandes catégories : **quantitatives** et **qualitatives**.
Cette distinction est importante, car elle détermine les techniques
statistiques et les méthodes de modélisation à employer.

### • Variables quantitatives

Il s’agit de variables **numériques continues ou discrètes** pouvant
faire l’objet de calculs statistiques classiques (moyenne, écart-type,
corrélation, etc.). Elles traduisent des mesures physiologiques ou
comportementales.

Les variables suivantes peuvent être considérées comme **quantitatives**
:

-   `Age` : Âge du patient\
-   `Cholesterol` : Taux de cholestérol\
-   `Heart.Rate` : Fréquence cardiaque\
-   `Exercise.Hours.Per.Week` : Heures d’exercice par semaine\
-   `Sedentary.Hours.Per.Day` : Heures passées assis par jour\
-   `Stress.Level` : Niveau de stress\
-   `Income` : Revenu annuel\
-   `BMI` : Indice de masse corporelle\
-   `Triglycerides` : Taux de triglycérides\
-   `Physical.Activity.Days.Per.Week` : Jours d’activité physique par
    semaine\
-   `Sleep.Hours.Per.Day` : Heures de sommeil par jour\
-   `Blood.Pressure` : À décomposer en `Systolic_BP` et `Diastolic_BP`
    pour une utilisation correcte

> Ces variables peuvent nécessiter un traitement préalable
> (normalisation, transformation logarithmique ou catégorisation) selon
> le modèle utilisé.


### • Variables qualitatives

Ces variables sont de nature **catégorielle** (nominale ou binaire) et
doivent être **encodées** avant d’être intégrées dans un modèle de
machine learning. Elles représentent des caractéristiques
comportementales, médicales ou sociodémographiques.

Les variables suivantes relèvent de cette catégorie :

-   `Sex` : Sexe du patient\
-   `Diabetes` : Diabète (Oui/Non)\
-   `Family.History` : Antécédents familiaux de maladies cardiaques\
-   `Smoking` : Tabagisme\
-   `Obesity` : Obésité\
-   `Alcohol.Consumption` : Consommation d’alcool\
-   `Diet` : Qualité de l’alimentation (Healthy, Average, Unhealthy)\
-   `Previous.Heart.Problems` : Antécédents cardiaques\
-   `Medication.Use` : Prise de médicaments\
-   `Country` : Pays de résidence\
-   `Continent` : Continent de résidence\
-   `Hemisphere` : Hémisphère géographique\
-   `Heart.Attack.Risk` : **Variable cible**, binaire (0 = Pas de
    risque, 1 = Risque)

> Certaines de ces variables (comme `Country`, `Continent` ou
> `Hemisphere`) seront exclues de la modélisation en raison de leur
> faible lien explicite avec le risque d’infarctus. La distinction entre
> ces deux types est essentielle pour : - orienter les **prétraitements
> nécessaires** (encodage, normalisation, transformation), - choisir les
> **méthodes de visualisation exploratoire** adaptées, - **adapter les
> algorithmes de classification** à la structure des données.

```{r}
str(data)
```

## 4. La variable target

L’objectif principal de ce projet étant de prédire si un patient présente un risque de crise cardiaque à partir de ses caractéristiques médicales, il est essentiel d’identifier avec précision la **variable cible** du modèle supervisé.
Dans notre jeu de données, la variable `Heart.Attack.Risk` remplit ce rôle. Elle indique si un individu est **à risque d’infarctus (1)** ou **non (0)**, en se basant sur une série de facteurs cliniques, comportementaux et démographiques.
Cette variable est de nature **binaire**, ce qui rend la problématique bien adaptée à une **approche de classification supervisée**.
Avant d’être utilisée dans les modèles, elle est convertie en facteur (`factor`) pour que les algorithmes de machine learning puissent la traiter comme une variable catégorielle.

Le code ci-dessous illustre cette conversion :
```{r}
target=as.factor(data$Heart.Attack.Risk)
class(target)
```

## 5. Nettoyage et vérification du data type
Les variables non pertinentes à l'analyse prédictive (comme Patient.ID, Country, Continent, Hemisphere) ont été retirées, car elles n’apportent pas d’information discriminante sur le risque d’infarctus.

De plus, la variable Blood.Pressure a été décomposée en deux variables numériques distinctes : Systolic_BP (pression systolique) et Diastolic_BP (pression diastolique), pour permettre une analyse fine.


```{r}
# Transformation de Blood Pressure en deux colonnes numériques 
data$Systolic_BP <- as.numeric(sub("/.*", "", data$Blood.Pressure))  # Extraire la pression systolique
data$Diastolic_BP <- as.numeric(sub(".*/", "", data$Blood.Pressure))  # Extraire la pression diastolique

# Supprime la colonne originale Blood Pressure
data <- data[, !(names(data) %in% "Blood.Pressure")]

```

### Modification du type des variables

Avant d’appliquer les algorithmes de classification supervisée, il est
indispensable d’adapter le format des variables à leur nature et à
l’exigence des modèles utilisés. Les variables quantitatives doivent
être **converties en numériques** (`numeric`) pour permettre les calculs
statistiques, tandis que les variables qualitatives doivent être
**converties en facteurs** (`factor`) afin que les modèles puissent
reconnaître leurs modalités comme des catégories distinctes.

Ainsi, les variables représentant des **mesures continues** telles que
l’âge, le taux de cholestérol, le BMI ou encore le nombre d’heures de
sport sont converties en format `numeric`.\
En parallèle, les variables catégorielles comme le sexe, la présence de
diabète, le régime alimentaire ou les antécédents médicaux sont
converties en `factor`.

Une discrétisation de l’âge a également été réalisée pour créer une
variable `Age_cat`, classant les individus en tranches d’âge (`Jeune`,
`Middle-aged`, `Senior`, `Elderly`) afin de faciliter certaines analyses
exploratoires et comparatives.

Le code suivant présente ces différentes opérations de typage :

```{r}
# **Variables numériques continues (mesures)**
data$Age_cat <- cut(data$Age, 
                    breaks = c(0, 30, 50, 70, 100), 
                    labels = c("Jeune", "Middle-aged", "Senior", "Elderly"))

data$Cholesterol <- as.numeric(data$Cholesterol)
data$Heart.Rate <- as.numeric(data$Heart.Rate)
data$BMI <- as.numeric(data$BMI)
data$Triglycerides <- as.numeric(data$Triglycerides)
data$Exercise.Hours.Per.Week <- as.numeric(data$Exercise.Hours.Per.Week)
data$Sedentary.Hours.Per.Day <- as.numeric(data$Sedentary.Hours.Per.Day)
data$Stress.Level <- as.numeric(data$Stress.Level)
data$Income <- as.numeric(data$Income)
data$Physical.Activity.Days.Per.Week <- as.numeric(data$Physical.Activity.Days.Per.Week)
data$Sleep.Hours.Per.Day <- as.numeric(data$Sleep.Hours.Per.Day)

#  **Variables catégorielles (facteurs)**
data$Sex <- as.factor(data$Sex)
data$Smoking <- as.factor(data$Smoking)
data$Diabetes <- as.factor(data$Diabetes)
data$Family.History <- as.factor(data$Family.History)
data$Diet <- as.factor(data$Diet)
data$Previous.Heart.Problems <- as.factor(data$Previous.Heart.Problems)
data$Medication.Use <- as.factor(data$Medication.Use)
data$Alcohol.Consumption <- as.factor(data$Alcohol.Consumption)
data$Obesity <- as.factor(data$Obesity)

str(data) 

```


## 6. Séparation des variables quantitatives et qualitatives

Dans le but de faciliter l’analyse statistique et la modélisation, les
variables du jeu de données ont été regroupées en deux sous-ensembles
distincts :

-   **Les variables quantitatives**, qui correspondent à des mesures
    numériques continues, sont destinées à des analyses statistiques
    classiques (statistiques descriptives, corrélations, boxplots,
    etc.).
-   **Les variables qualitatives**, qui regroupent les catégories (sexe,
    antécédents, comportement, etc.), feront l’objet d’analyses de
    fréquence et de visualisations spécifiques (diagrammes en barres,
    tables de contingence, etc.).

Ce regroupement permet une gestion plus efficace des données lors des
étapes suivantes du projet, notamment lors de l’analyse exploratoire, de
l’encodage, et de l’entraînement des modèles.

Le code suivant permet d’extraire ces deux sous-ensembles à l’aide du
package `dplyr` :

```{r}
library(dplyr)
quantitative_vars <- select(data, Age, Cholesterol, Heart.Rate, BMI, Triglycerides, 
                            Systolic_BP, Diastolic_BP, Exercise.Hours.Per.Week, Sedentary.Hours.Per.Day, 
                            Stress.Level, Physical.Activity.Days.Per.Week, Income
                            Sleep.Hours.Per.Day)


qualitative_vars <- select(data, Age_cat, Sex, Smoking, Diabetes, Obesity, Family.History, 
                           Diet, Previous.Heart.Problems, Alcohol.Consumption)


```

# II. Analyse exploratoire

## 1. Analyse univariée

### Variables quantitatives

Un résumé statistique a été effectué afin d’identifier les
distributions, valeurs aberrantes et échelles des variables continues :

```{r}
summary(quantitative_vars)
```
Les statistiques descriptives des variables quantitatives montrent que
les données sont globalement cohérentes et réalistes, sans valeurs
aberrantes majeures. On peut en dégager les observations suivantes :

-   **Âge** : les patients ont entre 18 et 90 ans, avec une moyenne
    d’environ **54 ans**. La distribution semble centrée sur la
    population adulte à senior, ce qui est cohérent avec une étude sur
    les risques cardiovasculaires.

-   **Cholestérol** : les valeurs varient de **120 à 400 mg/dL**, avec
    une moyenne de **260 mg/dL**, indiquant une population globalement
    en situation d’**hypercholestérolémie modérée à sévère**.

-   **Fréquence cardiaque (`Heart.Rate`)** : entre **40 et 110
    battements par minute**, avec une moyenne à **75 bpm**, ce qui reste
    dans les normes physiologiques générales.

-   **IMC (`BMI`)** : varie entre **18 et 40**, avec une moyenne
    d’environ **29**, ce qui correspond à une population en **surpoids
    voire obèse**, ce qui est attendu dans ce contexte médical.

-   **Triglycérides** : valeurs allant de **30 à 800 mg/dL**, avec une
    moyenne de **417 mg/dL**. Bien que la valeur maximale soit élevée,
    elle reste possible médicalement, et reflète un fort risque
    métabolique chez une partie de la population.

-   **Pression artérielle systolique et diastolique** : les valeurs
    moyennes sont de **135 mmHg** pour la systolique et **85 mmHg** pour
    la diastolique, proches des seuils de l’hypertension. Les valeurs
    minimales et maximales sont plausibles.

-   **Exercise.Hours.Per.Week** et **Physical.Activity.Days.Per.Week** :
    montrent une forte variabilité. La moyenne d’heures d’exercice est
    autour de **10 heures**, avec une distribution allant de **0 à près
    de 20 heures**, ce qui indique une **hétérogénéité importante dans
    les habitudes d’activité physique**.

-   **Sedentary.Hours.Per.Day** : varie de **0 à près de 12 heures**,
    avec une moyenne proche de **6 heures**, ce qui est élevé, mais
    réaliste pour une population potentiellement à risque.

-   **Stress.Level** : notée entre **1 et 10**, avec une moyenne autour
    de **5.5**, traduisant un **niveau de stress perçu modéré à élevé**.

-   **Sleep.Hours.Per.Day** : moyenne autour de **7 heures**, avec une
    distribution raisonnable (entre 4 et 10 heures), cohérente avec les
    recommandations de sommeil pour adultes.

-   **Income** : exprimé en unités monétaires (non spécifiées), varie de
    **\~80 000 à \~300 000**, avec une moyenne de **158 263**. Cette
    variable présente une distribution large, probablement asymétrique.


### Variables qualitatives

Le nombre d’observations tombées dans chacune des catégories

```{r}
sapply(qualitative_vars, table)
```
L’analyse des variables qualitatives met en évidence des répartitions
globalement équilibrées pour la majorité d’entre elles, tout en
soulignant certains déséquilibres notables qui reflètent le profil à
risque cardiovasculaire de la population étudiée.

La variable `Age_cat`, répartie en quatre tranches d’âge (Jeune,
Middle-aged, Senior, Elderly), montre une bonne répartition, légèrement
concentrée autour des personnes d’âge moyen et senior. Cela garantit une
représentation suffisante de chaque classe d’âge dans l’analyse.

La variable `Sex` présente un déséquilibre important, avec près de 70%
d’hommes. Cette surreprésentation du sexe masculin devra être prise en
compte dans l’interprétation des résultats, car elle peut influencer la
prédiction du risque.

Concernant le mode de vie et les antécédents médicaux : - La majorité
des individus sont **fumeurs** (près de 90%), **diabétiques** (65%) et
**obèses** (50%). Ces proportions très élevées soulignent le caractère à
haut risque de la population. - La variable `Family.History`, qui
indique la présence d’antécédents familiaux de maladies cardiaques, est
bien équilibrée (≈ 50/50), tout comme `Previous.Heart.Problems`,
`Medication.Use` et `Obesity`, permettant une modélisation comparative
efficace. - Les types de régime (`Diet`) sont également répartis de
manière homogène entre "Healthy", "Average" et "Unhealthy", ce qui
permet d’étudier leur impact potentiel sans biais de distribution. -
Enfin, `Alcohol.Consumption` révèle que 60% des individus consomment de
l’alcool, contre 40% qui n’en consomment pas, ce qui constitue une base
statistique suffisamment variée pour en évaluer les effets. Toutes ces
variables sont conservées dans l’analyse car elles sont soit bien
réparties, soit médicalement pertinentes, et permettent d’alimenter
efficacement la modélisation du risque d’infarctus.

### Variable Target

La variable cible est `Heart.Attack.Risk`

```{r}
table(target)
```

On observe 5624 observations dans la classe 0 (soit 64,2%) et 3139 dans
la classe 1 (35,8%). 

## 2. Analyse graphique
Interprétation : 
- Age : Q1 = 35, Médiane = 54, Q3 = 72
=> Les moustaches s’étendent de 18 à 90
=> Aucune valeur aberrante détectée
=> 50 % des individus ont un âge compris entre 35 et 72 ans. La médiane est de 54 ans. Toutes les valeurs sont incluses dans les moustaches, donc aucune valeur extrême n’est détectée.

- Cholesterol : Q1 = 192, Médiane = 259, Q3 = 330 et Pas de valeurs aberrantes
=> 50 % des données se situent entre 192 et 330. La médiane est de 259 mg/dL. L’ensemble des observations reste dans la plage normale définie par les moustaches.


- Heart Rate : Q1 = 57, Médiane = 75, Q3 = 93 , Min = 40, Max = 110 et	Pas de valeurs extrêmes
=> La moitié des individus ont une fréquence cardiaque entre 57 et 93 bpm. La médiane est à 75. Aucune valeur n’est considérée comme aberrante selon les critères du boxplot.

- xercise Hours Per Week: 	Q1 ≈ 5, Médiane ≈ 10, Q3 ≈ 15 ,Min ≈ 0, Max ≈ 20 et Pas de points au-delà des moustaches
=> 50 % des individus font entre 5 et 15 heures d’exercice par semaine. La médiane est de 10. La distribution est régulière et aucune valeur n’est jugée extrême.

- Stress Level : Q1 = 3, Médiane = 5, Q3 = 8 et Min = 1, Max = 10 Pas de valeurs extrêmes
=> Le niveau de stress est compris entre 3 et 8 pour la moitié des individus, avec une médiane de 5. Les moustaches couvrent l’ensemble des données, sans valeur aberrante.

- Sedentary Hours Per Day : Q1 ≈ 3, Médiane ≈ 6, Q3 ≈ 9,	Min ≈ 0, Max ≈ 12 et Pas de valeur extrême
=> Les heures passées assis par jour sont comprises entre 3 et 9 pour la moitié des individus. La médiane est à 6. Tous les individus restent dans les limites des moustaches.

-  Triglycerides: 	Q1 = 225.5, Médiane = 417, Q3 = 612, Min = 30, Max = 800 et pas de valeur aberrante détectée
=> Bien que les valeurs varient fortement, la distribution respecte les limites définies par les moustaches. Aucune valeur extrême détectée malgré un écart important.

- Physical Activity Days Per Week : 	Q1 = 2, Médiane = 3, Q3 = 5,	Min = 0, Max = 7
et	Aucun point hors des moustaches
=> 50 % des individus font entre 2 et 5 jours d’activité physique par semaine. La médiane est de 3 jours. Toutes les valeurs sont incluses dans les moustaches, donc aucune valeur extrême n’est présente.


- Sleep Hours Per Day :	Q1 = 5, Médiane = 7, Q3 = 9, Min = 4, Max = 10 et	Aucune valeur hors de l’intervalle attendu
=> La moitié des individus dorment entre 5 et 9 heures par jour. La médiane est de 7 heures. La distribution est équilibrée, sans valeur aberrante détectée.



- Heart Attack Risk :	Variable binaire : Min = 0, Max = 1, Médiane = 0,	Q1 = 0, Q3 = 1 et	Rien à détecter ici car ce n’est pas une variable continue
=> Cette variable prend uniquement les valeurs 0 ou 1. Le boxplot n’est pas pertinent ici car la variable est catégorique.


- systolic_BP :	Q1 = 112, Médiane = 135, Q3 = 158, Min = 90, Max = 180 et Toutes les valeurs sont dans les moustaches
=> La moitié des pressions systoliques se situent entre 112 et 158 mmHg, avec une médiane de 135. Aucune valeur extrême n’est présente selon la règle de 1,5×IQR.


- Diastolic_BP : 	Q1 = 72, Médiane = 85, Q3 = 98, Min = 60, Max = 110 et Pas de point en dehors des moustaches
=> La pression diastolique est comprise entre 72 et 98 mmHg pour 50 % des individus. La médiane est de 85. Aucun individu ne dépasse les limites définies par les moustaches.



### 2.2 Variables qualitatives : barplot

*Interprétation :* 

- Répartition de Age_cat selon le risque : 
On observe une légère surreprésentation des Seniors et Elderly dans le groupe à risque.Cela confirme que l’âge avancé est un facteur associé au risque d’infarctus, sans être totalement déterminant.


- Répartition de Sex selon le risque : 
Le nombre d’hommes dans le groupe à risque est nettement supérieur à celui des femmes.Le sexe masculin apparaît donc clairement corrélé au risque cardiaque dans ce jeu de données.


- Répartition de Smoking selon le risque :
La majorité des fumeurs se trouve dans le groupe à risque, tandis que les non-fumeurs sont peu présents dans ce groupe.Le tabagisme est ici un facteur fortement associé au risque, avec un potentiel explicatif élevé.

- Répartition de Diabetes selon le risque :
Les diabétiques sont nettement plus nombreux dans le groupe à risque que les non-diabétiques.Cette variable est donc fortement liée au risque de crise cardiaque et sera déterminante dans la modélisation.


- Répartition de Obesity selon le risque :
Les effectifs sont très similaires dans les deux groupes, que ce soit pour les obèses ou non.L’obésité n’apparaît pas ici comme un facteur discriminant net, bien qu’elle puisse jouer un rôle indirect.


- Répartition de Family.History selon le risque : 
Légère surreprésentation des antécédents familiaux dans le groupe à risque, mais la différence reste modérée.
Cette variable pourrait donc être informative, mais faiblement discriminante seule.

- Répartition de Diet selon le risque d’infarctus :
Les effectifs sont très proches entre les groupes, quelle que soit la qualité du régime alimentaire. La variable Diet ne montre pas de tendance marquée entre alimentation et risque cardiaque. Son pouvoir prédictif semble faible lorsqu’elle est prise isolément.

- Répartition de Previous.Heart.Problems selon le risque : 
Les patients ayant eu des antécédents cardiaques sont légèrement plus nombreux dans le groupe à risque. Cette variable est donc pertinente, mais la différence reste modérée.

- Répartition de Medication.Use selon le risque :
Les proportions sont quasiment identiques entre les deux groupes. La prise de médicament ne permet pas de discriminer clairement les patients à risque dans ce jeu de données.

- Répartition de Alcohol.Consumption selon le risque :
La consommation d’alcool est nettement plus élevée chez les patients à risque. Cette variable est donc fortement associée au risque d’infarctus, et mérite d’être prise en compte dans la modélisation.

- Répartition de Diet_encoded selon le risque : 
La variable encodée reprend fidèlement la distribution observée pour Diet.
Aucune différence significative entre les groupes.Comme sa version non encodée, elle semble peu informative seule.

## 3. Analyse multivariée
### 3.1 Gestion des valeurs manquantes

Avant de procéder à l'analyse multivariée, nous avons vérifié la présence de valeurs manquantes dans nos variables quantitatives et qualitatives. Cette étape est essentielle, car les valeurs manquantes peuvent fausser les analyses et nécessitent généralement des méthodes d'imputation ou d'exclusion.  

```{r}
# verification de valeurs manquantes
is.na(quantitative_vars[,1])
 is.na(qualitative_vars)

```

Après vérification, nous avons constaté que notre base de données **ne contient aucune valeur manquante**. Ainsi, aucune action corrective (imputation ou suppression) n’est requise, ce qui simplifie l’analyse à venir et garantit l’intégrité des données. 

### 3.2 Transformation des variables
Afin d’améliorer la qualité des données et garantir une meilleure performance des algorithmes d’analyse, nous appliquons plusieurs transformations aux variables quantitatives et qualitatives.

#### Normalisation par la méthode min max

La normalisation est une étape clé pour s'assurer que toutes les variables quantitatives sont comparables et influencent équitablement les analyses. Nous avons choisi la **méthode Min-Max**, qui transforme chaque variable pour qu’elle prenne des valeurs entre **0 et 1**.


```{r}
# Fonction de normalisation Min-Max
min_max_norm <- function(x) {
  return((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}

# Appliquer la normalisation sur les variables sélectionnées
quantitative_vars$Sedentary.Hours.Per.Day <- min_max_norm(quantitative_vars$Sedentary.Hours.Per.Day)
quantitative_vars$Exercise.Hours.Per.Week <- min_max_norm(quantitative_vars$Exercise.Hours.Per.Week)
quantitative_vars$Sleep.Hours.Per.Day <- min_max_norm(quantitative_vars$Sleep.Hours.Per.Day)
quantitative_vars$Stress.Level <- min_max_norm(quantitative_vars$Stress.Level)
quantitative_vars$Physical.Activity.Days.Per.Week <- min_max_norm(quantitative_vars$Physical.Activity.Days.Per.Week)

# Vérifier les résultats
summary(quantitative_vars)
```

>>Nous avons appliqué cette transformation aux variables suivantes :  
- **Sedentary Hours Per Day**  
- **Exercise Hours Per Week**  
- **Sleep Hours Per Day**  
- **Stress Level**  
- **Physical Activity Days Per Week**  

Après normalisation, nous avons vérifié les résultats à l'aide d'un résumé statistique de ces variables.

#### Encodage de la variable Diet
La variable Diet représente le type d’alimentation des individus avec trois catégories : 
-**Unhealthy** (Mauvaise alimentation)
-**Average** (Alimentation moyenne)
-**Healthy** (Bonne alimentation)
Afin d’utiliser cette variable dans des modèles de prédiction, nous devons la convertir en une variable numérique ordinale.

```{r}
# Transformer Diet en variable numérique ordinale correctement
qualitative_vars$Diet <- factor(qualitative_vars$Diet, 
                                levels = c("Unhealthy", "Average", "Healthy"), 
                                labels = c(0, 1, 2))

# Convertir en numérique
qualitative_vars$Diet <- as.numeric(as.character(qualitative_vars$Diet))

```

>>La catégorie "Unhealthy" est encodée par 0
La catégorie "Average" est encodée par 1
La catégorie "Healthy" est encodée par 2

Cette transformation permet d'intégrer correctement la variable **Diet** dans les analyses tout en conservant l’information sur l'ordre des catégories.  


### 3.2 Analyse Graphiques 
#### a Analyse des variables quantitatives

```{r}

library(ggplot2)
library(reshape2)

# Calcul de la matrice de corrélation
cor_mat <- cor(quantitative_vars, use = "complete.obs")

# Transformation en format long
melted_cor_mat <- melt(cor_mat)

# Création du heatmap amélioré
ggplot(data = melted_cor_mat, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", limits =c(-1,1)) +
  labs(title = "Correlation Heatmap", x = "", y = "", fill = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

**Interprétation** **Corrélations positives (rouge)** -Triglycérides & Systolic_BP (0.0051) et Triglycérides & Diastolic_BP (0.0005) : Très faible corrélation positive entre les triglycérides et la pression artérielle. Les valeurs sont proches de 0, donc pas significatives.

-Cholestérol & BMI (0.0173) : Une légère corrélation positive entre l’indice de masse corporelle (BMI) et le taux de cholestérol. une prise de poids peut être associée à une augmentation du cholestérol.

Stress Level & Age (0.0183) : Une légère corrélation montrant que le niveau de stress pourrait légèrement augmenter avec l’âge.

Mais la valeur est faible, donc pas de relation forte.

**Corrélations négatives (en bleu)**

-Stress Level & Physical Activity Days Per Week (-0.0074) : Une légère corrélation indiquant que plus une personne fait de l’activité physique, moins elle ressent de stress. Cependant, la valeur est très proche de 0, donc cette relation est faible.

-Triglycérides & Physical Activity Days Per Week (-0.0075) : Indique que les personnes qui pratiquent plus d’activité physique ont tendance à avoir un taux de triglycérides légèrement plus bas.

**Conclusion Générale** On observe aucune ucune relation forte entre les variables : Toutes les corrélations sont proches de 0, ce qui signifie qu’aucune variable ne semble influencer fortement une autre dans cet ensemble de données. Ce qui suggère que toutes les variables sont essentielles(pas de redondance dans les variables) pour prédire le risque d'infarctus.

#### b Variables qualitatives et target

```{R}
#library(ggplot2)
for(i in 1:ncol(qualitative_vars)){
var1 = as.character(qualitative_vars[,i])
var1 = as.factor(var1)
counts = NULL # il nous faut le nombre de passagers dans chacune des catégories
# définies par la variable quantitative et la target
for(j in levels(var1)){
for(k in levels(target)){
counts = rbind(counts, c(var1 = j, target = k,
patient_num = sum(target == k & var1 == j)))

}}
counts = as.data.frame(counts)
counts$var1 = as.factor(counts$var1)
counts$target = as.factor(counts$target)
counts$patient_num = as.numeric(counts$patient_num)
xlab_i = paste0(colnames(qualitative_vars)[i])
print(ggplot(data = counts, aes(x = var1, y = patient_num, fill = target)) +
geom_bar(stat = "identity", color="black") +
theme_minimal() + labs(x = xlab_i, fill = " Atteinte de risque cardiaque",
y = "Nombre de patients"))
}

```

**Interpretation**

#### 

```{r}
for(i in 1:ncol(qualitative_vars)){
var1 = as.character(qualitative_vars[,i])
var1[which(is.na(var1))] = "NA"
var1 = as.factor(var1)
counts = NULL   ##  il nous faut le nombre de passagers dans chacune des catégories
# définies par la variable quantitative et la target
for(j in levels(var1)){
for(k in levels(target)){
counts = rbind(counts, c(var1 = j, target = k,
patient_num = sum(target == k & var1 == j)))
}}
counts = as.data.frame(counts)
counts$var1 = as.factor(counts$var1)
counts$target = as.factor(counts$target)
counts$patient_num = as.numeric(counts$patient_num)
xlab_i = paste0(colnames(qualitative_vars)[i])
print(ggplot(data = counts, aes(x = var1, y = patient_num, fill = target)) +
geom_bar(stat = "identity", color="black", position = "fill") +
theme_minimal() + labs(x = xlab_i, fill = "Atteintes de risques",
y = "Proportion de patients"))
}
```

#### Création d'un data frame contenant les variables encodés
```{r}
write.csv(x = target, file = "attack_heart_pre_processed_target.csv", row.names = FALSE)
write.csv(x = quantitative_vars, file = "attack_heart_pre_processed_quantitative_vars.csv",
row.names = FALSE)
write.csv(x = qualitative_vars, file = "attack_heart_pre_processed_qualitative_vars.csv",
row.names = FALSE)
```


# III Modèles d'évaluation
## 1. Methodes des k_plus proches voisins
### 1.1 Méthode de Holdout

####  Partition du Jeu de Données (Holdout Split)

Dans cette étape, nous réalisons une séparation aléatoire de notre jeu de données en deux sous-ensembles : un ensemble d’apprentissage (Train) et un ensemble de test (Test).

Pour assurer la reproductibilité des résultats, nous commençons par fixer une graine aléatoire avec `set.seed(1500)`. Cela garantit que la sélection des observations reste la même à chaque exécution du code.

Nous déterminons ensuite le nombre total d’observations dans notre jeu de données à l’aide de nrow(predictive_vars). Puis, nous utilisons la fonction sample() pour tirer aléatoirement 80 % des observations qui constitueront l’ensemble d’apprentissage.

Les données sont ensuite partitionnées comme suit :

predictive_vars_train et target_train : contiennent les variables prédictives et la cible pour l’ensemble d’apprentissage (80 % des données).

predictive_vars_test et target_test : contiennent les observations restantes (20 % des données), qui serviront à évaluer la performance du modèle.

Ce découpage est essentiel pour éviter le surajustement et garantir que le modèle apprend sur un ensemble d’entraînement tout en étant évalué sur un jeu de test indépendant.

```{r}
set.seed(1500)  # Pour la reproductibilité
nobs <- nrow(predictive_vars)
inTrain <- sample(1:nobs, size = round(0.8 * nobs), replace = FALSE)

predictive_vars_test    = predictive_vars[-inTrain,]
target_test             = target[-inTrain]
predictive_vars_train   = predictive_vars[inTrain,]
target_train            = target[inTrain]
```
Afin d’évaluer efficacement la performance du modèle et d’optimiser ses hyperparamètres, nous effectuons une partition du jeu de données en trois sous-ensembles :

-   **Ensemble d’apprentissage (Train - 80 %)** : utilisé pour entraîner le modèle.\
-   **Ensemble de validation (Validation - 10 %)** : permet d’ajuster les hyperparamètres et d’éviter le surajustement.\
-   **Ensemble de test (Test - 10 %)** : sert à évaluer la performance finale du modèle sur des données totalement indépendantes.

Pour garantir la reproductibilité de la répartition des observations, nous fixons une graine aléatoire. Ensuite, nous sélectionnons aléatoirement **10 % des observations pour l’ensemble de test** et **10 % pour l’ensemble de validation**, en veillant à ce que ces ensembles ne se chevauchent pas. Le reste des données est attribué à l’ensemble d’apprentissage (80 %).

Enfin, les variables cibles des ensembles d’apprentissage, de validation et de test sont converties en **facteurs**, ce qui est essentiel pour garantir un traitement correct des données catégoriques dans les modèles supervisés.

Cette séparation en trois ensembles permet d’obtenir une évaluation plus robuste et d’améliorer la généralisation du modèle en évitant un sur-apprentissage.

```{r}
set.seed(1237)  
nobs = nrow(predictive_vars)  

# Sélection des indices pour test et validation
inTest       = sample(1:nobs, size = round(0.1 * nobs), replace = FALSE)
notinTest    = setdiff(1:nobs, inTest)
inValidation = sample(notinTest, size = round(0.1 * nobs), replace = FALSE)
inTrain      = setdiff(notinTest, inValidation)  # Le reste pour l'entraînement (80%)

# Création des datasets
predictive_vars_test       = predictive_vars[inTest,]
target_test                = target[inTest]

predictive_vars_validation = predictive_vars[inValidation,]
target_validation          = target[inValidation]

predictive_vars_train      = predictive_vars[inTrain,]
target_train               = target[inTrain]

target_train = as.factor(target_train)
target_validation = as.factor(target_validation)
target_pred = as.factor(target_pred)
target_test = as.factor(target_test)

```

#### Sélection de l’Hyperparamètre k

Dans cette étape, nous cherchons à optimiser le nombre de voisins **k** pour la méthode des k plus proches voisins (kNN). L’hyperparamètre **k** joue un rôle essentiel dans la performance du modèle :\
- Un **k trop petit** risque d'entraîner un modèle trop sensible aux variations locales des données, pouvant causer du **surajustement**.\
- Un **k trop grand** peut lisser excessivement les prédictions et réduire la capacité du modèle à capturer des motifs locaux dans les données.

Nous testons donc différentes valeurs de **k** (de 1 à 20) et évaluons les performances du modèle sur l’ensemble de validation en utilisant plusieurs métriques :\
- **Accuracy** : mesure globale des prédictions correctes.\
- **Recall (Sensibilité)** : proportion des instances positives correctement identifiées.\
- **Precision** : proportion des prédictions positives qui sont réellement correctes.

Pour chaque valeur de **k**, nous utilisons l’algorithme **kNN** pour prédire les classes des observations dans l’ensemble de validation, puis nous évaluons les performances du modèle avec une **matrice de confusion**.

Enfin, nous regroupons les résultats sous forme d’un tableau, ce qui nous permet d’identifier la valeur optimale de **k** en fonction des performances observées.

```{r, results = 'hide'}
library(class) ## chargement du jeu de donnees
library(caret) ### pour les metriques d'evaluation 

# Définition de la grille des valeurs de K à tester
KGrid         = 1:20
accuracy_val  = NULL
recall_val    = NULL
precision_val = NULL


# Boucle pour tester chaque valeur de K
for(k in KGrid){
  
  # Prédiction avec kNN
  target_pred = knn(train = predictive_vars_train, 
                    test = predictive_vars_validation, 
                    cl = target_train, 
                    k = k) 
  
  # Conversion en facteur pour correspondre à target_validation
  target_pred = as.factor(target_pred)
# Vérifier et harmoniser les niveaux des facteurs
target_pred = factor(target_pred, levels = levels(target_validation))

  
  # Calcul des métriques avec confusionMatrix()
  cm = confusionMatrix(target_pred, target_validation, positive = "1")

  accuracy_val  = c(accuracy_val, cm$overall["Accuracy"])
  recall_val    = c(recall_val, cm$byClass["Sensitivity"])  # Sensitivity = Recall
  precision_val = c(precision_val, cm$byClass["Precision"])
}

# Afficher les résultats sous forme de tableau
results = data.frame(K = KGrid, Accuracy = accuracy_val, Recall = recall_val, Precision = precision_val)
print(results)

```

> Les résultats montrent que l'**accuracy** augmente progressivement avec **k**, atteignant un maximum d'environ **0.6187** pour **k = 20**. Cependant, cette augmentation de l'accuracy s'accompagne d'une baisse du **recall**, suggérant que le modèle devient plus conservateur à mesure que **k** augmente, identifiant moins de cas positifs. En revanche, la **précision** augmente légèrement avec **k**, atteignant **0.3939** pour **k = 20**.

Cela montre qu'il existe un compromis entre **recall** et **precision** :\
- Pour maximiser le **recall**, c'est-à-dire identifier un maximum de cas positifs, il serait préférable de choisir une valeur plus faible de **k**.\
- En revanche, pour améliorer la **précision** des prédictions positives, une valeur plus élevée de **k** serait plus appropriée.

Le choix optimal de **k** dépend donc de l'importance relative que l'on accorde à chaque métrique dans le contexte de notre application spécifique.

> > pour se faire nous avons donc opté pour un k plus élevé, privilégiant ainsi une meilleure stabilité du modèle et une précision accrue, même si cela se fait au prix d’une légère diminution du recall. Ce choix permet d'éviter un modèle trop sensible aux variations locales des données, réduisant ainsi le risque de surajustement.

### Les différentes métriques en fonction des valeurs de K étudiées

Afin de mieux visualiser l'impact du choix de **k** sur les performances du modèle, nous avons représenté graphiquement l'évolution des différentes métriques(**accuracy**, **recall** et **precision**) en fonction des valeurs de **k** testées.

```{r, out.width="50%", fig.align = "center", eval = FALSE}
plot(KGrid, accuracy_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Accuracy", col="purple")
plot(KGrid, recall_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Recall", col="red")
plot(KGrid, precision_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Precision", col="blue")
```
#### Interpretation des graphiques

> > -   **Accuracy (précision globale du modèle)** : représentée en violet, elle montre une tendance globale à l'augmentation avec **k**, atteignant un plateau autour de **k = 20**.\
> > -   **Recall (sensibilité)** : en rouge, cette métrique diminue lorsque **k** augmente. Cela signifie que le modèle détecte de moins en moins de cas positifs à mesure que **k** croît.\
> > -   **Precision** : en bleu, elle tend à augmenter légèrement avec **k**, ce qui suggère que les prédictions positives deviennent plus fiables.

#### Performance du classifieur final


## 1. Arbre de classification

### Variables explicatives

### Partage du jeu de données

### 1.1 Phase d'apprentissage

Installation des packages `rpart`et `rpart.plot`

```{r, eval = FALSE}
install.packages("rpart")
install.packages("rpart.plot")
```

Chargement des librairies

```{r, results = "hide", message = FALSE}
library(rpart)
library(rpart.plot)
?rpart
?rpart.plot
```

### 1.2. Entrainement d'un arbre de classification.


### 1.3. Tracé de l'arbre de décision

### 1.4. Phase de test


### 1.5. Prédiction de la target avec l'arbre de décision sur le jeu de données de test



#  Chargement des packages nécessaires
library(caret)

# S'assurer que les jeux d'entraînement et de test sont bien des data.frames
predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_test <- as.data.frame(predictive_vars_test)
table(target_train)
prop.table(table(target_train))
#  Entraînement du modèle de régression logistique
logit_model <- glm(target_train ~ ., 
                   data = predictive_vars_train, 
                   weights = ifelse(target_train == 1, 1.5, 1),  # Pondère plus fort les 1
                   family = binomial(link = "logit"))

#  Prédictions : probabilité que la classe soit 1
target_pred_prob <- predict(logit_model, newdata = predictive_vars_test, type = "response")

#  Transformation des probabilités en classes (seuil 0.5)
target_pred_logit <- ifelse(target_pred_prob > 0.5, 1, 0)

#  Conversion en facteur avec les bons niveaux
target_pred_logit <- factor(target_pred_logit, levels = levels(target_test))

# Vérification des longueurs
stopifnot(length(target_pred_logit) == length(target_test))

#  Matrice de confusion
confusionMatrix(target_pred_logit, target_test)
```
***Interpretation***
	-	Accuracy : 0.6218 => Le modèle est correct dans 62% des cas. C’est un peu moins bon que le taux de majorité (0.6446), donc pas très performant globalement.
	-	Sensitivity : 0.9389 => Excellente détection des patients sans risque (classe 0). Presque tous les vrais 0 sont bien identifiés.
	-	Specificity : 0.0465 =>  Très mauvaise détection des patients à risque (classe 1). Le modèle ne reconnaît presque aucun vrai cas de risque.
	-	Pos Pred Value : 0.641 => Quand le modèle prédit “0”, c’est bon à 64% du temps.
	-	Neg Pred Value : 0.296 => Quand le modèle prédit “1”, il se trompe souvent.
	-	Balanced Accuracy : 0.4927 => En dessous de 0.5 = pire qu’un tirage au sort binaire.
	-	Kappa : -0.0179 => Accord entre le modèle et la vérité à peine mieux qu’un hasard.

Conclusion :

Ton modèle prédit quasiment toujours la classe 0, et il a du mal à détecter les vrais patients à risque. Il est biaisé vers la classe majoritaire.

