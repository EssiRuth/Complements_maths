---
title: "Projet"
author: "ruth"
date: "2025-03-07"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction

## Contexte

L’augmentation du nombre de cas de maladies cardiovasculaires à
l’échelle mondiale représente un défi majeur pour les systèmes de santé.
Parmi celles-ci, l’infarctus du myocarde figure comme l'une des
principales causes de mortalité. La prévention, via une détection
précoce des risques, constitue une voie cruciale d’intervention. Dans ce
contexte, l’analyse de données cliniques par des méthodes de
classification supervisée peut permettre de prédire efficacement le
risque d’infarctus chez un patient, et ainsi d'orienter plus rapidement
les stratégies de prise en charge.

## Objectif

Le présent projet vise à développer un modèle prédictif permettant
d’estimer, à partir de données médicales et comportementales, la
probabilité qu’un individu présente un risque d’infarctus. L’objectif
est double : D’une part, explorer les variables influentes dans la
prédiction du risque cardiovasculaire. D’autre part, évaluer les
performances de modèles de classification supervisée (arbre de décision,
k-NN et classification naive bayésienne) implémentés dans le langage R.

# I. Préparation des données

## 1. Présentation du jeu de données

Le jeu de données utilisé comprend 8763 individus et 26 variables, dont
une variable cible Heart.Attack.Risk indiquant si l’individu présente
(1) ou non (0) un risque d’infarctus. Les observations portent sur des
aspects cliniques (cholestérol, pression artérielle, triglycérides,
etc.), comportementaux (tabagisme, activité physique, régime
alimentaire) et démographiques (âge, sexe, revenu, pays de résidence).

```{r}
data=read.csv("~/Téléchargements/heart_attack_prediction_dataset.csv")
dim(data)
colnames(data)
```

Un aperçu des variables montre une diversité de types (quantitatif
continu, qualitatif binaire ou nominal, ordinal...), ce qui implique un
traitement différencié lors du prétraitement.

## 2. Description et pertinence des variables

Le jeu de données comprend 26 variables. Celles-ci couvrent des
informations cliniques, comportementales, démographiques et
géographiques. Afin de garantir la qualité de la modélisation, il est
essentiel d’identifier les variables les plus pertinentes, et d’en
exclure certaines qui peuvent introduire du bruit ou de la redondance.

Le tableau ci-dessous présente une description synthétique de chaque
variable, son type, ainsi que son niveau de pertinence pour le projet de
prédiction du risque d’infarctus :

```{r message=FALSE, warning=FALSE}
library(knitr)

variable_info <- data.frame(
  Variable = c("Patient.ID", "Age", "Sex", "Cholesterol", "Blood.Pressure", "Heart.Rate",
               "Diabetes", "Family.History", "Smoking", "Obesity", "Alcohol.Consumption",
               "Exercise.Hours.Per.Week", "Diet", "Previous.Heart.Problems",
               "Medication.Use", "Stress.Level", "Sedentary.Hours.Per.Day", "Income",
               "BMI", "Triglycerides", "Physical.Activity.Days.Per.Week",
               "Sleep.Hours.Per.Day", "Country", "Continent", "Hemisphere", "Heart.Attack.Risk"),
  
  Description = c("Identifiant unique du patient", "Âge du patient", "Sexe (Male/Female)",
                  "Taux de cholestérol total", "Tension artérielle (ex: 120/80)",
                  "Fréquence cardiaque", "Présence de diabète", 
                  "Antécédents familiaux de maladies cardiaques",
                  "Fumeur ou non", "Obésité", "Consommation d’alcool",
                  "Heures d’exercice par semaine", "Qualité de l’alimentation",
                  "Antécédents cardiaques", "Utilisation de médicaments", 
                  "Niveau de stress", "Heures assises par jour",
                  "Revenu annuel", "Indice de masse corporelle",
                  "Taux de triglycérides", "Jours d’activité physique/semaine",
                  "Heures de sommeil par jour", "Pays de résidence", 
                  "Continent de résidence", "Hémisphère", 
                  "Risque d’infarctus (0 ou 1)"),
  
  Type = c("Identifiant", "Quantitative", "Qualitative", "Quantitative", "Texte",
           "Quantitative", "Binaire", "Binaire", "Binaire", "Binaire", "Binaire",
           "Quantitative", "Qualitative", "Binaire", "Binaire", "Quantitative",
           "Quantitative", "Quantitative", "Quantitative", "Quantitative",
           "Quantitative", "Quantitative", "Qualitative", "Qualitative",
           "Qualitative", "Binaire"),
  
  Pertinence = c("Non pertinente", "Très pertinente", "Pertinente", "Pertinente",
                 " À transformer", " Pertinente", " Très pertinente",
                 " Pertinente", " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Très pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Pertinente", " Pertinente", " Pertinente", " Moyennement pertinente",
                 " Non pertinente", " Non pertinente", " Non pertinente",
                 " Variable cible")
)

kable(variable_info, caption = "Tableau 1 : Description des variables et évaluation de leur pertinence", align = "l", booktabs = TRUE)

```

> Certaines variables comme `Patient.ID`, `Country`, `Continent` et
> `Hemisphere` seront écartées, car elles n’apportent pas de valeur
> informative dans la prédiction du risque cardiaque.

## 3. Typologie des variables

Les variables présentes dans le jeu de données peuvent être regroupées
en deux grandes catégories : **quantitatives** et **qualitatives**.
Cette distinction est importante, car elle détermine les techniques
statistiques et les méthodes de modélisation à employer.

### • Variables quantitatives

Il s’agit de variables **numériques continues ou discrètes** pouvant
faire l’objet de calculs statistiques classiques (moyenne, écart-type,
corrélation, etc.). Elles traduisent des mesures physiologiques ou
comportementales.

Les variables suivantes peuvent être considérées comme **quantitatives**
:

-   `Age` : Âge du patient\
-   `Cholesterol` : Taux de cholestérol\
-   `Heart.Rate` : Fréquence cardiaque\
-   `Exercise.Hours.Per.Week` : Heures d’exercice par semaine\
-   `Sedentary.Hours.Per.Day` : Heures passées assis par jour\
-   `Stress.Level` : Niveau de stress\
-   `Income` : Revenu annuel\
-   `BMI` : Indice de masse corporelle\
-   `Triglycerides` : Taux de triglycérides\
-   `Physical.Activity.Days.Per.Week` : Jours d’activité physique par
    semaine\
-   `Sleep.Hours.Per.Day` : Heures de sommeil par jour\
-   `Blood.Pressure` : À décomposer en `Systolic_BP` et `Diastolic_BP`
    pour une utilisation correcte

> Ces variables peuvent nécessiter un traitement préalable
> (normalisation, transformation logarithmique ou catégorisation) selon
> le modèle utilisé.

------------------------------------------------------------------------

### • Variables qualitatives

Ces variables sont de nature **catégorielle** (nominale ou binaire) et
doivent être **encodées** avant d’être intégrées dans un modèle de
machine learning. Elles représentent des caractéristiques
comportementales, médicales ou sociodémographiques.

Les variables suivantes relèvent de cette catégorie :

-   `Sex` : Sexe du patient\
-   `Diabetes` : Diabète (Oui/Non)\
-   `Family.History` : Antécédents familiaux de maladies cardiaques\
-   `Smoking` : Tabagisme\
-   `Obesity` : Obésité\
-   `Alcohol.Consumption` : Consommation d’alcool\
-   `Diet` : Qualité de l’alimentation (Healthy, Average, Unhealthy)\
-   `Previous.Heart.Problems` : Antécédents cardiaques\
-   `Medication.Use` : Prise de médicaments\
-   `Country` : Pays de résidence\
-   `Continent` : Continent de résidence\
-   `Hemisphere` : Hémisphère géographique\
-   `Heart.Attack.Risk` : **Variable cible**, binaire (0 = Pas de
    risque, 1 = Risque)

> Certaines de ces variables (comme `Country`, `Continent` ou
> `Hemisphere`) seront exclues de la modélisation en raison de leur
> faible lien explicite avec le risque d’infarctus. La distinction entre
> ces deux types est essentielle pour : - orienter les **prétraitements
> nécessaires** (encodage, normalisation, transformation), - choisir les
> **méthodes de visualisation exploratoire** adaptées, - **adapter les
> algorithmes de classification** à la structure des données.

```{r}
str(data)
```

## 4. La variable target

Notre variable target sera la variable Heart.Attack.Risk qui est une
variable binaire prenant la valeur 0 ou 1.

```{r}
target=as.factor(data$Heart.Attack.Risk)
class(target)
```

## 5. Nettoyage et vérification du data type

Les variables non pertinentes à l'analyse prédictive (comme Patient.ID,
Country, Continent, Hemisphere) ont été retirées, car elles n’apportent
pas d’information discriminante sur le risque d’infarctus.

De plus, la variable Blood.Pressure a été décomposée en deux variables
numériques distinctes : Systolic_BP (pression systolique) et
Diastolic_BP (pression diastolique), pour permettre une analyse fine.

```{r}
# Transformer Blood Pressure en deux colonnes numériques sans mutate()
data$Systolic_BP <- as.numeric(sub("/.*", "", data$Blood.Pressure))  # Extraire la pression systolique
data$Diastolic_BP <- as.numeric(sub(".*/", "", data$Blood.Pressure))  # Extraire la pression diastolique

# Supprimer la colonne originale Blood Pressure
data <- data[, !(names(data) %in% "Blood.Pressure")]

```

### Modification du type des variables

Avant d’appliquer les algorithmes de classification supervisée, il est
indispensable d’adapter le format des variables à leur nature et à
l’exigence des modèles utilisés. Les variables quantitatives doivent
être **converties en numériques** (`numeric`) pour permettre les calculs
statistiques, tandis que les variables qualitatives doivent être
**converties en facteurs** (`factor`) afin que les modèles puissent
reconnaître leurs modalités comme des catégories distinctes.

Ainsi, les variables représentant des **mesures continues** telles que
l’âge, le taux de cholestérol, le BMI ou encore le nombre d’heures de
sport sont converties en format `numeric`.\
En parallèle, les variables catégorielles comme le sexe, la présence de
diabète, le régime alimentaire ou les antécédents médicaux sont
converties en `factor`.

Une discrétisation de l’âge a également été réalisée pour créer une
variable `Age_cat`, classant les individus en tranches d’âge (`Jeune`,
`Middle-aged`, `Senior`, `Elderly`) afin de faciliter certaines analyses
exploratoires et comparatives.

Le code suivant présente ces différentes opérations de typage :

```{r}
# **Variables numériques continues (mesures)**
data$Age_cat <- cut(data$Age, 
                   breaks = c(0, 30, 50, 70, 100), 
                  labels = c("Jeune", "Middle-aged", "Senior", "Elderly"))

data$Cholesterol <- as.numeric(data$Cholesterol)
data$Heart.Rate <- as.numeric(data$Heart.Rate)
data$BMI <- as.numeric(data$BMI)
data$Triglycerides <- as.numeric(data$Triglycerides)
data$Exercise.Hours.Per.Week <- as.numeric(data$Exercise.Hours.Per.Week)
data$Sedentary.Hours.Per.Day <- as.numeric(data$Sedentary.Hours.Per.Day)
data$Stress.Level <- as.numeric(data$Stress.Level)
data$Income <- as.numeric(data$Income)
data$Physical.Activity.Days.Per.Week <- as.numeric(data$Physical.Activity.Days.Per.Week)
data$Sleep.Hours.Per.Day <- as.numeric(data$Sleep.Hours.Per.Day)

#  **Variables catégorielles (facteurs)**
data$Sex <- as.factor(data$Sex)
data$Smoking <- as.factor(data$Smoking)
data$Diabetes <- as.factor(data$Diabetes)
data$Family.History <- as.factor(data$Family.History)
data$Diet <- as.factor(data$Diet)
data$Previous.Heart.Problems <- as.factor(data$Previous.Heart.Problems)
data$Medication.Use <- as.factor(data$Medication.Use)
data$Alcohol.Consumption <- as.factor(data$Alcohol.Consumption)
data$Obesity <- as.factor(data$Obesity)

# Hypertension (1 si pression systolique > 140 ou diastolique > 90)
data$Hypertension <- ifelse(data$Systolic_BP > 140 | data$Diastolic_BP > 90, 1, 0)

# Catégorie de BMI
data$BMI_cat <- cut(data$BMI, breaks = c(0, 18.5, 24.9, 29.9, 100),
                    labels = c("Maigreur", "Normal", "Surpoids", "Obésité"))

# Ratio activité/sédentarité
data$Active_ratio <- data$Exercise.Hours.Per.Week / (data$Sedentary.Hours.Per.Day + 1)

# Qualité du sommeil (mauvais si <5 ou >9h)
data$Poor_sleep <- ifelse(data$Sleep.Hours.Per.Day < 5 | data$Sleep.Hours.Per.Day > 9, 1, 0)


str(data) 

```

## 6. Séparation des variables quantitatives et qualitatives

Dans le but de faciliter l’analyse statistique et la modélisation, les
variables du jeu de données ont été regroupées en deux sous-ensembles
distincts :

-   **Les variables quantitatives**, qui correspondent à des mesures
    numériques continues, sont destinées à des analyses statistiques
    classiques (statistiques descriptives, corrélations, boxplots,
    etc.).
-   **Les variables qualitatives**, qui regroupent les catégories (sexe,
    antécédents, comportement, etc.), feront l’objet d’analyses de
    fréquence et de visualisations spécifiques (diagrammes en barres,
    tables de contingence, etc.).

Ce regroupement permet une gestion plus efficace des données lors des
étapes suivantes du projet, notamment lors de l’analyse exploratoire, de
l’encodage, et de l’entraînement des modèles.

Le code suivant permet d’extraire ces deux sous-ensembles à l’aide du
package `dplyr` :

```{r}
library(dplyr)
quantitative_vars <- select(data, Cholesterol, Heart.Rate, BMI, Triglycerides, 
                            Systolic_BP, Diastolic_BP, Exercise.Hours.Per.Week, Sedentary.Hours.Per.Day, 
                            Stress.Level, Physical.Activity.Days.Per.Week, Income,
                            Sleep.Hours.Per.Day)


qualitative_vars <- select(data, Age_cat, Sex, Smoking, Diabetes, Obesity, Family.History, 
                           Diet, Previous.Heart.Problems, Alcohol.Consumption, Medication.Use)
str(qualitative_vars)
str(quantitative_vars)


```

# II. Analyse exploratoire

## 1. Analyse univariée

### Variables quantitatives

Un résumé statistique a été effectué afin d’identifier les
distributions, valeurs aberrantes et échelles des variables continues :

```{r}
summary(quantitative_vars)
```

Les statistiques descriptives des variables quantitatives montrent que
les données sont globalement cohérentes et réalistes, sans valeurs
aberrantes majeures. On peut en dégager les observations suivantes :

-   **Cholestérol** : les valeurs varient de **120 à 400 mg/dL**, avec
    une moyenne de **260 mg/dL**, indiquant une population globalement
    en situation d’**hypercholestérolémie modérée à sévère**.

-   **Fréquence cardiaque (`Heart.Rate`)** : entre **40 et 110
    battements par minute**, avec une moyenne à **75 bpm**, ce qui reste
    dans les normes physiologiques générales.

-   **IMC (`BMI`)** : varie entre **18 et 40**, avec une moyenne
    d’environ **29**, ce qui correspond à une population en **surpoids
    voire obèse**, ce qui est attendu dans ce contexte médical.

-   **Triglycérides** : valeurs allant de **30 à 800 mg/dL**, avec une
    moyenne de **417 mg/dL**. Bien que la valeur maximale soit élevée,
    elle reste possible médicalement, et reflète un fort risque
    métabolique chez une partie de la population.

-   **Pression artérielle systolique et diastolique** : les valeurs
    moyennes sont de **135 mmHg** pour la systolique et **85 mmHg** pour
    la diastolique, proches des seuils de l’hypertension. Les valeurs
    minimales et maximales sont plausibles.

-   **Exercise.Hours.Per.Week** et **Physical.Activity.Days.Per.Week** :
    montrent une forte variabilité. La moyenne d’heures d’exercice est
    autour de **10 heures**, avec une distribution allant de **0 à près
    de 20 heures**, ce qui indique une **hétérogénéité importante dans
    les habitudes d’activité physique**.

-   **Sedentary.Hours.Per.Day** : varie de **0 à près de 12 heures**,
    avec une moyenne proche de **6 heures**, ce qui est élevé, mais
    réaliste pour une population potentiellement à risque.

-   **Stress.Level** : notée entre **1 et 10**, avec une moyenne autour
    de **5.5**, traduisant un **niveau de stress perçu modéré à élevé**.

-   **Sleep.Hours.Per.Day** : moyenne autour de **7 heures**, avec une
    distribution raisonnable (entre 4 et 10 heures), cohérente avec les
    recommandations de sommeil pour adultes.

-   **Income** : exprimé en unités monétaires (non spécifiées), varie de
    **\~80 000 à \~300 000**, avec une moyenne de **158 263**. Cette
    variable présente une distribution large, probablement asymétrique.

### Variables qualitatives

Le nombre d’observations tombées dans chacune des catégories

```{r}
sapply(qualitative_vars, table)
```

L’analyse des variables qualitatives met en évidence des répartitions
globalement équilibrées pour la majorité d’entre elles, tout en
soulignant certains déséquilibres notables qui reflètent le profil à
risque cardiovasculaire de la population étudiée.

La variable `Age_cat`, répartie en quatre tranches d’âge (Jeune,
Middle-aged, Senior, Elderly), montre une bonne répartition, légèrement
concentrée autour des personnes d’âge moyen et senior. Cela garantit une
représentation suffisante de chaque classe d’âge dans l’analyse.

La variable `Sex` présente un déséquilibre important, avec près de 70%
d’hommes. Cette surreprésentation du sexe masculin devra être prise en
compte dans l’interprétation des résultats, car elle peut influencer la
prédiction du risque.

Concernant le mode de vie et les antécédents médicaux :

-   La majorité des individus sont **fumeurs** (près de 90%),
    **diabétiques** (65%) et **obèses** (50%). Ces proportions très
    élevées soulignent le caractère à haut risque de la population. - La
    variable `Family.History`, qui indique la présence d’antécédents
    familiaux de maladies cardiaques, est bien équilibrée (≈ 50/50),
    tout comme `Previous.Heart.Problems`, `Medication.Use` et `Obesity`,
    permettant une modélisation comparative efficace.

-   Les types de régime (`Diet`) sont également répartis de manière
    homogène entre "Healthy", "Average" et "Unhealthy", ce qui permet
    d’étudier leur impact potentiel sans biais de distribution. - Enfin,
    `Alcohol.Consumption` révèle que 60% des individus consomment de
    l’alcool, contre 40% qui n’en consomment pas, ce qui constitue une
    base statistique suffisamment variée pour en évaluer les effets.
    Toutes cesvariables sont conservées dans l’analyse car elles sont
    soit bien réparties, soit médicalement pertinentes, et permettent
    d’alimenter efficacement la modélisation du risque d’infarctus.

### Variable Target

La variable cible est `Heart.Attack.Risk`

```{r}

table(target)
```

On observe 5624 observations dans la classe 0 (soit 64,2%) et 3139 dans
la classe 1 (35,8%). Cette répartition présente un léger déséquilibre,
mais reste suffisamment équilibrée pour permettre une modélisation
fiable des deux classes.

## 2. Analyse graphique

### Variables quantitatives : boxplot

```{r}
for(i in 1:ncol(quantitative_vars)){
  boxplot(quantitative_vars[,i] ~ target, main=paste("Boxplot de", colnames(quantitative_vars)[i]),
          col = c("pink", "lightblue"), xlab = "Risque d'infarctus")
}
```

#### Interprétation

-   Cholesterol : Q1 = 192, Médiane = 259, Q3 = 330 et Pas de valeurs
    aberrantes =\> 50 % des données se situent entre 192 et 330. La
    médiane est de 259 mg/dL. L’ensemble des observations reste dans la
    plage normale définie par les moustaches.

-   Heart Rate : Q1 = 57, Médiane = 75, Q3 = 93 , Min = 40, Max = 110 et
    Pas de valeurs extrêmes =\> La moitié des individus ont une
    fréquence cardiaque entre 57 et 93 bpm. La médiane est à 75. Aucune
    valeur n’est considérée comme aberrante selon les critères du
    boxplot.

-   xercise Hours Per Week: Q1 ≈ 5, Médiane ≈ 10, Q3 ≈ 15 ,Min ≈ 0, Max
    ≈ 20 et Pas de points au-delà des moustaches =\> 50 % des individus
    font entre 5 et 15 heures d’exercice par semaine. La médiane est
    de 10. La distribution est régulière et aucune valeur n’est jugée
    extrême.

-   Stress Level : Q1 = 3, Médiane = 5, Q3 = 8 et Min = 1, Max = 10 Pas
    de valeurs extrêmes =\> Le niveau de stress est compris entre 3 et 8
    pour la moitié des individus, avec une médiane de 5. Les moustaches
    couvrent l’ensemble des données, sans valeur aberrante.

-   Sedentary Hours Per Day : Q1 ≈ 3, Médiane ≈ 6, Q3 ≈ 9, Min ≈ 0, Max
    ≈ 12 et Pas de valeur extrême =\> Les heures passées assis par jour
    sont comprises entre 3 et 9 pour la moitié des individus. La médiane
    est à 6. Tous les individus restent dans les limites des moustaches.

-   Triglycerides: Q1 = 225.5, Médiane = 417, Q3 = 612, Min = 30, Max =
    800 et pas de valeur aberrante détectée =\> Bien que les valeurs
    varient fortement, la distribution respecte les limites définies par
    les moustaches. Aucune valeur extrême détectée malgré un écart
    important.

-   Physical Activity Days Per Week : Q1 = 2, Médiane = 3, Q3 = 5, Min =
    0, Max = 7 et Aucun point hors des moustaches =\> 50 % des individus
    font entre 2 et 5 jours d’activité physique par semaine. La médiane
    est de 3 jours. Toutes les valeurs sont incluses dans les
    moustaches, donc aucune valeur extrême n’est présente.

-   Sleep Hours Per Day : Q1 = 5, Médiane = 7, Q3 = 9, Min = 4, Max = 10
    et Aucune valeur hors de l’intervalle attendu =\> La moitié des
    individus dorment entre 5 et 9 heures par jour. La médiane est de 7
    heures. La distribution est équilibrée, sans valeur aberrante
    détectée.

-   Heart Attack Risk : Variable binaire : Min = 0, Max = 1, Médiane =
    0, Q1 = 0, Q3 = 1 et Rien à détecter ici car ce n’est pas une
    variable continue =\> Cette variable prend uniquement les valeurs 0
    ou 1. Le boxplot n’est pas pertinent ici car la variable est
    catégorique.

-   systolic_BP : Q1 = 112, Médiane = 135, Q3 = 158, Min = 90, Max = 180
    et Toutes les valeurs sont dans les moustaches =\> La moitié des
    pressions systoliques se situent entre 112 et 158 mmHg, avec une
    médiane de 135. Aucune valeur extrême n’est présente selon la règle
    de 1,5×IQR.

-   Diastolic_BP : Q1 = 72, Médiane = 85, Q3 = 98, Min = 60, Max = 110
    et Pas de point en dehors des moustaches =\> La pression diastolique
    est comprise entre 72 et 98 mmHg pour 50 % des individus. La médiane
    est de 85. Aucun individu ne dépasse les limites définies par les
    moustaches.

### Variables qualitatives et target : barplot

```{R}
#library(ggplot2)
for(i in 1:ncol(qualitative_vars)){
var1 = as.character(qualitative_vars[,i])
var1 = as.factor(var1)
counts = NULL # il nous faut le nombre de passagers dans chacune des catégories
# définies par la variable quantitative et la target
for(j in levels(var1)){
for(k in levels(target)){
counts = rbind(counts, c(var1 = j, target = k,
patient_num = sum(target == k & var1 == j)))

}}
counts = as.data.frame(counts)
counts$var1 = as.factor(counts$var1)
counts$target = as.factor(counts$target)
counts$patient_num = as.numeric(counts$patient_num)
xlab_i = paste0(colnames(qualitative_vars)[i])
print(ggplot(data = counts, aes(x = var1, y = patient_num, fill = target)) +
geom_bar(stat = "identity", color="black") +
theme_minimal() + labs(x = xlab_i, fill = " Atteinte de risque cardiaque",
y = "Nombre de patients"))
}

```

#### Interprétation

- Répartition de Age_cat selon le risque : 
On observe une légère surreprésentation des Seniors et Elderly dans le groupe à risque.Cela confirme que l’âge avancé est un facteur associé au risque d’infarctus, sans être totalement déterminant.

- Répartition de Sex selon le risque : 
Le nombre d’hommes dans le groupe à risque est nettement supérieur à celui des femmes.Le sexe masculin apparaît donc clairement corrélé au risque cardiaque dans ce jeu de données.

- Répartition de Smoking selon le risque :
La majorité des fumeurs se trouve dans le groupe à risque, tandis que les non-fumeurs sont peu présents dans ce groupe.Le tabagisme est ici un facteur fortement associé au risque, avec un potentiel explicatif élevé.

- Répartition de Diabetes selon le risque :
Les diabétiques sont nettement plus nombreux dans le groupe à risque que les non-diabétiques.Cette variable est donc fortement liée au risque de crise cardiaque et sera déterminante dans la modélisation.

- Répartition de Obesity selon le risque :
Les effectifs sont très similaires dans les deux groupes, que ce soit pour les obèses ou non.L’obésité n’apparaît pas ici comme un facteur discriminant net, bien qu’elle puisse jouer un rôle indirect.

- Répartition de Family.History selon le risque : 
Légère surreprésentation des antécédents familiaux dans le groupe à risque, mais la différence reste modérée.
Cette variable pourrait donc être informative, mais faiblement discriminante seule.

- Répartition de Diet selon le risque d’infarctus :
Les effectifs sont très proches entre les groupes, quelle que soit la qualité du régime alimentaire. La variable Diet ne montre pas de tendance marquée entre alimentation et risque cardiaque. Son pouvoir prédictif semble faible lorsqu’elle est prise isolément.

- Répartition de Previous.Heart.Problems selon le risque : 
Les patients ayant eu des antécédents cardiaques sont légèrement plus nombreux dans le groupe à risque. Cette variable est donc pertinente, mais la différence reste modérée.

- Répartition de Medication.Use selon le risque :
Les proportions sont quasiment identiques entre les deux groupes. La prise de médicament ne permet pas de discriminer clairement les patients à risque dans ce jeu de données.

- Répartition de Alcohol.Consumption selon le risque :
La consommation d’alcool est nettement plus élevée chez les patients à risque. Cette variable est donc fortement associée au risque d’infarctus, et mérite d’être prise en compte dans la modélisation.

- Répartition de Diet_encoded selon le risque : 
La variable encodée reprend fidèlement la distribution observée pour Diet.
Aucune différence significative entre les groupes.Comme sa version non encodée, elle semble peu informative seule.


## 3. Analyse multivariée
### 3.1 Gestion des valeurs manquantes

Avant de procéder à l'analyse multivariée, nous avons vérifié la présence de valeurs manquantes dans nos variables quantitatives et qualitatives. Cette étape est essentielle, car les valeurs manquantes peuvent fausser les analyses et nécessitent généralement des méthodes d'imputation ou d'exclusion.  

```{r}
# verification de valeurs manquantes
is.na(quantitative_vars[,1])
 is.na(qualitative_vars)

```

Après vérification, nous avons constaté que notre base de données **ne contient aucune valeur manquante**. Ainsi, aucune action corrective (imputation ou suppression) n’est requise, ce qui simplifie l’analyse à venir et garantit l’intégrité des données. 

### 3.2 Analyse Graphique 
#### Analyse des variables quantitatives

```{r}

library(ggplot2)
library(reshape2)

# Calcul de la matrice de corrélation
cor_mat <- cor(quantitative_vars, use = "complete.obs")

# Transformation en format long
melted_cor_mat <- melt(cor_mat)

# Création du heatmap amélioré
ggplot(data = melted_cor_mat, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", limits =c(-1,1)) +
  labs(title = "Correlation Heatmap", x = "", y = "", fill = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

**Interprétation** 
**Corrélations positives (rouge)** -Triglycérides & Systolic_BP (0.0051) et Triglycérides & Diastolic_BP (0.0005) : Très faible corrélation positive entre les triglycérides et la pression artérielle. Les valeurs sont proches de 0, donc pas significatives.

-Cholestérol & BMI (0.0173) : Une légère corrélation positive entre l’indice de masse corporelle (BMI) et le taux de cholestérol. une prise de poids peut être associée à une augmentation du cholestérol.

Stress Level & Age (0.0183) : Une légère corrélation montrant que le niveau de stress pourrait légèrement augmenter avec l’âge.

Mais la valeur est faible, donc pas de relation forte.

**Corrélations négatives (en bleu)**

-Stress Level & Physical Activity Days Per Week (-0.0074) : Une légère corrélation indiquant que plus une personne fait de l’activité physique, moins elle ressent de stress. Cependant, la valeur est très proche de 0, donc cette relation est faible.

-Triglycérides & Physical Activity Days Per Week (-0.0075) : Indique que les personnes qui pratiquent plus d’activité physique ont tendance à avoir un taux de triglycérides légèrement plus bas.

**Conclusion Générale** On observe aucune ucune relation forte entre les variables : Toutes les corrélations sont proches de 0, ce qui signifie qu’aucune variable ne semble influencer fortement une autre dans cet ensemble de données. Ce qui suggère que toutes les variables sont essentielles(pas de redondance dans les variables) pour prédire le risque d'infarctus.

#### Variables qualitatives et target
```{r}
for(i in 1:ncol(qualitative_vars)){
var1 = as.character(qualitative_vars[,i])
var1[which(is.na(var1))] = "NA"
var1 = as.factor(var1)
counts = NULL   ##  il nous faut le nombre de passagers dans chacune des catégories
# définies par la variable quantitative et la target
for(j in levels(var1)){
for(k in levels(target)){
counts = rbind(counts, c(var1 = j, target = k,
patient_num = sum(target == k & var1 == j)))
}}
counts = as.data.frame(counts)
counts$var1 = as.factor(counts$var1)
counts$target = as.factor(counts$target)
counts$patient_num = as.numeric(counts$patient_num)
xlab_i = paste0(colnames(qualitative_vars)[i])
print(ggplot(data = counts, aes(x = var1, y = patient_num, fill = target)) +
geom_bar(stat = "identity", color="black", position = "fill") +
theme_minimal() + labs(x = xlab_i, fill = "Atteintes de risques",
y = "Proportion de patients"))
}
```

# III Modèles d'évaluation
## 1. Methodes des k_plus proches voisins
### 1.1 Méthode de Holdout

####  Partition du Jeu de Données (Holdout Split)

Dans cette étape, nous réalisons une séparation aléatoire de notre jeu de données en deux sous-ensembles : un ensemble d’apprentissage (Train) et un ensemble de test (Test).

Pour assurer la reproductibilité des résultats, nous commençons par fixer une graine aléatoire avec `set.seed(1500)`. Cela garantit que la sélection des observations reste la même à chaque exécution du code.

Nous déterminons ensuite le nombre total d’observations dans notre jeu de données à l’aide de nrow(predictive_vars). Puis, nous utilisons la fonction sample() pour tirer aléatoirement 80 % des observations qui constitueront l’ensemble d’apprentissage.

Les données sont ensuite partitionnées comme suit :

predictive_vars_train et target_train : contiennent les variables prédictives et la cible pour l’ensemble d’apprentissage (80 % des données).

predictive_vars_test et target_test : contiennent les observations restantes (20 % des données), qui serviront à évaluer la performance du modèle.

Ce découpage est essentiel pour éviter le surajustement et garantir que le modèle apprend sur un ensemble d’entraînement tout en étant évalué sur un jeu de test indépendant.

```{r}
set.seed(1237) #pour la reproductibilité 
nobs <- nrow(predictive_vars)
inTrain <- sample(1:nobs, size = round(0.8 * nobs), replace = FALSE)

predictive_vars_test    = predictive_vars[-inTrain,]
target_test             = target[-inTrain]
predictive_vars_train   = predictive_vars[inTrain,]
target_train            = target[inTrain]
```


Afin d’évaluer efficacement la performance du modèle et d’optimiser ses hyperparamètres, nous effectuons une partition du jeu de données en trois sous-ensembles :

-   **Ensemble d’apprentissage (Train - 80 %)** : utilisé pour entraîner le modèle.\
-   **Ensemble de validation (Validation - 10 %)** : permet d’ajuster les hyperparamètres et d’éviter le surajustement.\
-   **Ensemble de test (Test - 10 %)** : sert à évaluer la performance finale du modèle sur des données totalement indépendantes.

Pour garantir la reproductibilité de la répartition des observations, nous fixons une graine aléatoire. Ensuite, nous sélectionnons aléatoirement **10 % des observations pour l’ensemble de test** et **10 % pour l’ensemble de validation**, en veillant à ce que ces ensembles ne se chevauchent pas. Le reste des données est attribué à l’ensemble d’apprentissage (80 %).

Enfin, les variables cibles des ensembles d’apprentissage, de validation et de test sont converties en **facteurs**, ce qui est essentiel pour garantir un traitement correct des données catégoriques dans les modèles supervisés.

Cette séparation en trois ensembles permet d’obtenir une évaluation plus robuste et d’améliorer la généralisation du modèle en évitant un sur-apprentissage.

```{r}
set.seed(1237)
inTest = sample(1:nobs, size = round(0.1 * nobs), replace = FALSE)
notinTest = (1:nobs)[-inTest]
inValidation = sample(notinTest, size = round(0.1 * nobs), replace = FALSE)
c(inTest, inValidation)

# Création des datasets
predictive_vars_test       = predictive_vars[inTest,]
target_test                = target[inTest]

predictive_vars_validation = predictive_vars[inValidation,]
target_validation          = target[inValidation]

predictive_vars_train      = predictive_vars[inTrain,]
target_train               = target[inTrain]

```


#### Sélection de l’Hyperparamètre k

Dans cette étape, nous cherchons à optimiser le nombre de voisins **k** pour la méthode des k plus proches voisins (kNN). L’hyperparamètre **k** joue un rôle essentiel dans la performance du modèle :\
- Un **k trop petit** risque d'entraîner un modèle trop sensible aux variations locales des données, pouvant causer du **surajustement**.\
- Un **k trop grand** peut lisser excessivement les prédictions et réduire la capacité du modèle à capturer des motifs locaux dans les données.

Nous testons donc différentes valeurs de **k** (de 1 à 20) et évaluons les performances du modèle sur l’ensemble de validation en utilisant plusieurs métriques :\
- **Accuracy** : mesure globale des prédictions correctes.\
- **Recall (Sensibilité)** : proportion des instances positives correctement identifiées.\
- **Precision** : proportion des prédictions positives qui sont réellement correctes.

Pour chaque valeur de **k**, nous utilisons l’algorithme **kNN** pour prédire les classes des observations dans l’ensemble de validation, puis nous évaluons les performances du modèle avec une **matrice de confusion**.

Enfin, nous regroupons les résultats sous forme d’un tableau, ce qui nous permet d’identifier la valeur optimale de **k** en fonction des performances observées.

```{r, results = 'hide'}
library(class) ## chargement du jeu de donnees
library(caret) ### pour les metriques d'evaluation 

# Définition de la grille des valeurs de K à tester
KGrid         = 1:20
accuracy_val  = NULL
recall_val    = NULL
precision_val = NULL


# Boucle pour tester chaque valeur de K
for(k in KGrid){
  
  # Prédiction avec kNN
  target_pred = knn(train = predictive_vars_train, 
                    test = predictive_vars_validation, 
                    cl = target_train, 
                    k = k) 
  
  # Conversion en facteur pour correspondre à target_validation
  target_pred = as.factor(target_pred)
# Vérifier et harmoniser les niveaux des facteurs
target_pred = factor(target_pred, levels = levels(target_validation))

  
  # Calcul des métriques avec confusionMatrix()
  cm = confusionMatrix(target_pred, target_validation, positive = "1")

  accuracy_val  = c(accuracy_val, cm$overall["Accuracy"])
  recall_val    = c(recall_val, cm$byClass["Sensitivity"])  # Sensitivity = Recall
  precision_val = c(precision_val, cm$byClass["Precision"])
}

# Afficher les résultats sous forme de tableau
results = data.frame(K = KGrid, Accuracy = accuracy_val, Recall = recall_val, Precision = precision_val)
print(results)

```

> Les résultats obtenus montrent que l’accuracy diminue progressivement avec k, atteignant un maximum d’environ 0.8973 pour k = 1. Cependant, cette diminution de l'accuracy s'accompagne d'une baisse du recall, indiquant que le modèle devient plus conservateur à mesure que k augmente, identifiant moins de cas positifs. En revanche, la précision tend à s’améliorer légèrement avec un k plus élevé.

On observe ainsi un compromis entre recall et précision :

Un faible k (ex: k = 1-3) permet d’identifier plus de cas positifs (meilleur recall), mais avec un risque de surajustement aux données d'entraînement.

Un k plus élevé (ex: k > 10) stabilise le modèle et augmente la précision, mais réduit la sensibilité aux cas positifs.

Afin d’obtenir un équilibre entre précision, recall et stabilité du modèle, nous avons opté pour k = 4.

Avec k = 4, nous obtenons une accuracy de 0.6792,

Un recall de 0.4531, assurant une détection correcte d’une bonne partie des cas positifs,

Une précision de 0.5777, permettant de limiter les fausses alertes.

### Les différentes métriques en fonction des valeurs de K étudiées

Afin de mieux visualiser l'impact du choix de **k** sur les performances du modèle, nous avons représenté graphiquement l'évolution des différentes métriques(**accuracy**, **recall** et **precision**) en fonction des valeurs de **k** testées.

```{r, out.width="50%", fig.align = "center", eval = FALSE}
plot(KGrid, accuracy_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Accuracy", col="purple")
plot(KGrid, recall_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Recall", col="red")
plot(KGrid, precision_val, type = 'l', xlab = "K, nombre de voisins", ylab = "Precision", col="blue")
```
#### Interpretation des graphiques

> -Accuracy (précision globale du modèle) : représentée en violet, elle atteint son maximum pour k = 1, puis diminue progressivement avant de se stabiliser autour de k = 20. Cela reflète le compromis entre un modèle très flexible (faible k) et un modèle plus stable mais moins précis (grand k).

-Recall (sensibilité) : en rouge, cette métrique décroît avec l’augmentation de k, indiquant que le modèle détecte de moins en moins de cas positifs à mesure que k augmente. Un faible k favorise donc la détection des cas positifs, mais au risque d'une plus grande variabilité des résultats.

-Précision : en bleu, elle tend à s’améliorer légèrement avec k, ce qui signifie que les prédictions positives deviennent plus fiables, bien que cela se fasse au détriment du recall. Un k plus élevé privilégie donc des prédictions plus précises mais moins nombreuses.

#### Performance du classifieur final
Pour évaluer la performance de notre modèle, nous avons entraîné un classifieur k-plus proches voisins (k-NN) avec k = 4, valeur choisie pour assurer un bon équilibre entre recall et précision.

```{r, results = 'hide', eval = FALSE}
rbind(predictive_vars_train, predictive_vars_validation)
c(target_train, target_validation)
```

```{r, eval = FALSE}
target_pred = knn(train = rbind(predictive_vars_train, predictive_vars_validation),
test = predictive_vars_test,
cl = c(target_train, target_validation),
k = 4)
accuracy_test = mean(target_pred == target_test)
recall_test = recall(data = target_pred, target_test,
relevant = "0")
precision_test = precision(target_pred, target_test,
relevant = "0")
confusionMatrix(target_pred, target_test)$table
accuracy_test
recall_test
precision_test

```

> L'évaluation sur l’ensemble de test donne les résultats suivants :

- Accuracy : 55.62 % → Le modèle classe correctement un peu plus de la moitié des observations. Cette performance reste modérée et indique que des améliorations sont possibles.

- Recall : 71.97 % → Le modèle détecte environ 72 % des cas positifs. Il est donc assez efficace pour identifier les éléments appartenant à la classe cible, mais il peut encore en manquer.

- Précision : 61.84 % → Lorsqu’il prédit un élément comme appartenant à la classe cible, il a environ 62 % de chances d’avoir raison. Cela signifie qu’il reste un certain nombre de fausses alertes.

Ces résultats traduisent le compromis réalisé lors du choix de k. En optant pour une valeur modérée, nous avons limité la sensibilité du modèle aux fluctuations locales des données, tout en évitant un surajustement. Toutefois, l’accuracy relativement faible indique que le modèle pourrait être amélioré.

### Utisation de la partie Test pour évaluer le classifieur final

Afin d’évaluer la robustesse du modèle, nous avons appliqué notre classifieur k-NN (k = 4) sur l’ensemble d’entraînement et de validation regroupés. Cette approche permet de vérifier dans quelle mesure le modèle s’adapte aux données sur lesquelles il a été appris.

```{r}
target_pred_train = knn(train = rbind(predictive_vars_train, predictive_vars_validation),
                         test = rbind(predictive_vars_train, predictive_vars_validation),
                         cl = c(target_train, target_validation),
                         k = 4)

accuracy_train = mean(target_pred_train == c(target_train, target_validation))
recall_train = recall(data = target_pred_train, c(target_train, target_validation), relevant = "0")
precision_train = precision(target_pred_train, c(target_train, target_validation), relevant = "0")

confusionMatrix(target_pred_train, c(target_train, target_validation))$table
accuracy_train
recall_train
precision_train

```
Les performances obtenues sont les suivantes :

- Accuracy : 72.34 % : Le modèle classe correctement environ 72 % des observations. Cette performance est nettement meilleure que celle obtenue sur l’ensemble de test, ce qui suggère un possible surajustement du modèle aux données d'entraînement.

- Recall : 84.31 % : Le modèle détecte plus de 84 % des cas positifs. Il a donc une forte capacité à identifier la classe cible, ce qui est un bon point si l’objectif est de minimiser les faux négatifs.

- Précision : 75.75 % : Lorsqu’il prédit un élément comme appartenant à la classe cible, il a environ 76 % de chances d’avoir raison. Cette valeur est plus élevée que celle obtenue sur l’ensemble de test, ce qui indique que le modèle fonctionne bien sur les données connues mais peut avoir plus de mal à généraliser.

**Analyse des résultats**
La différence notable entre les performances sur l’ensemble d’entraînement et celles obtenues sur l’ensemble de test indique que le modèle pourrait souffrir d’un légèr surajustement : il s’adapte bien aux données d’apprentissage mais perd en généralisation sur de nouvelles données et cela peut être aussi  dû probablement à notre base de données qui presente moins de personnes aynat de risques..

Pour améliorer la performance globale du modèle et éviter cet effet, plusieurs pistes peuvent être envisagées :

-Tester d’autres valeurs de k pour identifier un meilleur compromis entre recall et précision.

-Introduire une validation croisée afin d’avoir une évaluation plus fiable des performances du modèle.

-Expérimenter d’autres méthodes de classification comme les arbres de décision,naïve bayesienne pour comparer les résultats.

## 1.2 Méthode de cross-validation


```{r, eval = FALSE}
set.seed(1357)
inTest                  = sample(1:nobs, size = round(0.1 * nobs), replace = FALSE)
predictive_vars_test    = predictive_vars[inTest,]
target_test             = target[inTest]
predictive_vars_remain  = predictive_vars[-inTest,] # ce qu'on partagera en Train 
# et Validation à chaque CV itération
target_remain           = target[-inTest]

nobs_remain = nrow(predictive_vars_remain)
nfolds      = 10
KGrid       = 1:20
  
folds   = NULL
for(i in 1:nobs_remain){
  folds = c(folds, sample(1:nfolds, 1, replace = FALSE))
}

accuracy_fold = NULL
recall_fold = NULL
precision_fold = NULL

for(fold in 1: nfolds){ # pour faire les itérations de la CV
  inFold                      = which(folds == fold)
  predictive_vars_train       = predictive_vars_remain[-inFold,]  # pour entrainer le modèle
  predictive_vars_validation  = predictive_vars_remain[inFold,]   # pour évaluer le modèle
  target_train                = target_remain[-inFold]
  target_validation           = target_remain[inFold]
    
 
  accuracy_k  = NULL
  recall_k    = NULL
  precision_k = NULL
  for(k in KGrid){ # pour chaque valeur de K dans KGrid on a fait "l'apprentissage" du 
    # classifieur, puis on prédit la réponse des individidus dans la partie Validation, 
    # et enfin on compare les prédictions aux vraies valeurs de la réponse
    target_pred = knn(train = predictive_vars_train, test = predictive_vars_validation, 
                      cl = target_train, k = k) 
      
    accuracy_k      = c(accuracy_k, mean(target_pred == target_validation))
    recall_k        = c(recall_k, 
                        recall(data = target_pred, target_validation, 
                               relevant = "0"))
    precision_k     = c(precision_k, 
                        precision(target_pred, target_validation, 
                                  relevant = "0"))
    }
    
    accuracy_fold = rbind(accuracy_fold, accuracy_k)
    recall_fold = rbind(recall_fold, recall_k)
    precision_fold = rbind(precision_fold, precision_k)
  }
rownames(accuracy_fold) = paste0("fold_",1:nfolds)
rownames(recall_fold) = paste0("fold_",1:nfolds)
rownames(precision_fold) = paste0("fold_",1:nfolds)
  
colnames(accuracy_fold) = paste0("K_", KGrid)
colnames(recall_fold) = paste0("K_", KGrid)
colnames(precision_fold) = paste0("K_", KGrid)
```

```{r}
# 10-fold cross validated mean accuracy pour chaque valeur de K
mean_accuracy_k = colMeans(accuracy_fold)

# 10-fold cross validated mean recall pour chaque valeur de K
mean_recall_k = colMeans(recall_fold)

# 10-fold cross validated mean precision pour chaque valeur de K
mean_precision_k = colMeans(precision_fold)

# Affichage des résultats
mean_accuracy_k
mean_recall_k
mean_precision_k
```

### Graphes 10-fold mean accuracy en fonction des valeurs de K
```{r, out.width="50%", fig.align = "center"}
# Charger ggplot2 pour les graphiques
library(ggplot2)

# Création d'un data.frame pour ggplot
results = data.frame(
  K = KGrid,
  Accuracy = mean_accuracy_k,
  Recall = mean_recall_k,
  Precision = mean_precision_k
)

# Graphique de l'accuracy
ggplot(results, aes(x = K, y = Accuracy)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  labs(title = "10-Fold Mean Accuracy vs. K", x = "K", y = "Mean Accuracy") +
  theme_minimal()

# Graphique du recall
ggplot(results, aes(x = K, y = Recall)) +
  geom_line(color = "red") +
  geom_point(color = "red") +
  labs(title = "10-Fold Mean Recall vs. K", x = "K", y = "Mean Recall") +
  theme_minimal()

# Graphique de la précision
ggplot(results, aes(x = K, y = Precision)) +
  geom_line(color = "green") +
  geom_point(color = "green") +
  labs(title = "10-Fold Mean Precision vs. K", x = "K", y = "Mean Precision") +
  theme_minimal()


```
##3 2.4 Performance du classifieur final
```{r, eval = FALSE}
library(caret)
library(class)

# Application du modèle kNN avec le meilleur K trouvé (ex: K = 3, à ajuster)
best_k = 20 # Mettre la meilleure valeur de K trouvée
target_pred = knn(train = predictive_vars_remain,
                  test = predictive_vars_test, 
                  cl = target_remain, 
                  k = best_k)

# Calcul des performances sur le jeu de test
accuracy_test  = mean(target_pred == target_test)
recall_test    = recall(data = target_pred, target_test, 
                        relevant = "0")
precision_test = precision(target_pred, target_test, 
                           relevant = "0")

# Affichage des résultats
accuracy_test
recall_test 
precision_test

```

## 3. Arbre de classification

### Variables explicatives

### Partage du jeu de données

### 1.1 Phase d'apprentissage

Installation des packages `rpart`et `rpart.plot`

```{r, eval = FALSE}
install.packages("rpart")
install.packages("rpart.plot")
```

Chargement des librairies

```{r, results = "hide", message = FALSE}
library(rpart)
library(rpart.plot)
?rpart
?rpart.plot
```

### 1.2. Phase d'apprentissage

```{r, results = "hide", message = FALSE}
library(rpart)
library(rpart.plot)
?rpart
?rpart.plot
```
```{r, eval = FALSE}
target_train <- as.factor(target_train)
#classifTree = rpart(target_train ~ ., data = predictive_vars_train, 
    #                method = "class")
predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_test <- as.data.frame(predictive_vars_test)

classifTree <- rpart(target_test ~ ., data = predictive_vars_test, 
                     method = "class",
                     control = rpart.control(minsplit = 10, cp = 0.001, maxdepth = 5))

```

### 3.3. Tracé de l'arbre de décision

```{r, eval = FALSE}
rpart.plot(classifTree, type = 3, extra = 2, fallen.leaves = TRUE,  
           main = "Arbre de prédiction du risque de crise cardiaque") 
```

### 3.4. Prédiction de la target avec l'arbre de décision sur le jeu de données de test
```{r, eval = FALSE}
# Prédiction sur le jeu de test
target_pred = predict(classifTree, predictive_vars_test, type = "class")

# Calcul des métriques
cm = confusionMatrix(target_pred, target_test, positive = "1")

# Affichage des résultats
accuracy_test  = cm$overall["Accuracy"]
recall_test    = cm$byClass["Sensitivity"]  # Sensitivity = Recall
precision_test = cm$byClass["Precision"]

accuracy_test
recall_test
precision_test


## 2. La régression logistique

La régression logistique ne traite pas les variables catégorielles
telles qu'elles comme la méthode des k plus proches voisins. Il traite
que des variables de type numérique donc on va réutiliser la data frame
`predictive_vars`de la méthode des k plus proches voisins.

```{r}

library(caret)
```

```{r}
logit_model <- glm(target_train ~ ., data = predictive_vars_train, family = binomial)

target_pred_prob <- predict(logit_model, newdata = predictive_vars_test, type = "response")

#  Transformation des probabilités en classes (seuil 0.5)
target_pred_logit <- ifelse(target_pred_prob > 0.5, 1, 0)

#  Conversion en facteur avec les bons niveaux
target_pred_logit <- factor(target_pred_logit, levels = levels(target_test))

# Vérification des longueurs
stopifnot(length(target_pred_logit) == length(target_test))

#  Matrice de confusion
confusionMatrix(target_pred_logit, target_test, positive="1")

conf_matrix <- confusionMatrix(target_pred_logit, target_test, positive = "1")

# Extraire les métriques
accuracy_test   <- conf_matrix$overall["Accuracy"]
recall_test     <- conf_matrix$byClass["Sensitivity"]  # Rappel
precision_test  <- conf_matrix$byClass["Precision"]
f1_score        <- 2 * (precision_test * recall_test) / (precision_test + recall_test)

# Affichage
cat("Accuracy  :", accuracy_test, "\n")
cat("Recall    :", recall_test, "\n")
cat("Precision :", precision_test, "\n")
cat("F1-Score  :", f1_score, "\n")
```
***Interpretation***
	-	Accuracy : 0.6518 => Le modèle est correct dans 62% des cas. C’est un peu moins bon que le taux de majorité (0.6446), donc pas très performant globalement.
	-	Sensitivity : 0.9389 => Excellente détection des patients sans risque (classe 0). Presque tous les vrais 0 sont bien identifiés.
	-	Specificity : 0.0465 =>  Très mauvaise détection des patients à risque (classe 1). Le modèle ne reconnaît presque aucun vrai cas de risque.
	-	Pos Pred Value : 0.641 => Quand le modèle prédit “0”, c’est bon à 64% du temps.
	-	Neg Pred Value : 0.296 => Quand le modèle prédit “1”, il se trompe souvent.
	-	Balanced Accuracy : 0.4927 => En dessous de 0.5 = pire qu’un tirage au sort binaire.
	-	Kappa : -0.0179 => Accord entre le modèle et la vérité à peine mieux qu’un hasard.

Conclusion :

Le modèle prédit quasiment toujours la classe 0, et il a du mal à détecter les vrais patients à risque. Il est biaisé vers la classe majoritaire.


### 2.1. La courbe de ROC

```{r}
library(pROC) # chargement de la librairie pROC pour le tracé de la courbe de ROC
```

```{r}
# target_pred_prob doit contenir les probabilités de prédire la classe "1"
roc_curve <- roc(response = target_test, predictor = target_pred_prob)

# Tracer la courbe
plot(roc_curve, col = "blue", main = "Courbe ROC - Régression Logistique", lwd = 2)

# AUC
auc_val <- auc(roc_curve)
cat("AUC :", auc_val, "\n")

```

## 3. Arbre de classification

### 3.1. Variables explicatives

Avant de construire l’arbre de classification, nous avons regroupé
l’ensemble des variables explicatives — à la fois quantitatives et
qualitatives — dans un seul objet nommé predictive_vars. Une
vérification de la structure (str) permet de s'assurer que les types de
données sont correctement interprétés (par exemple, les variables
qualitatives sont bien des facteurs, les quantitatives sont numériques).
Cette étape garantit une préparation adéquate des données en vue de
l'apprentissage du modèle.

```{r}
predictive_vars = cbind(quantitative_vars, qualitative_vars)
head(predictive_vars)
str(predictive_vars)
```

### 3.2. Partage du jeu de données

Afin d’évaluer correctement les performances de notre modèle, nous avons
séparé le jeu de données en deux sous-ensembles : 80% des données ont
été utilisées pour l'entraînement du modèle (train), 20% restantes ont
été conservées pour le test final (test).

Le partage s’effectue de manière aléatoire mais reproductible grâce à la
fonction set.seed().

```{r, eval = FALSE}

set.seed(1234)
nobs    = nrow(qualitative_vars) 
inTrain = sample(1:nobs, size = round(0.8 * nobs), replace = FALSE)

predictive_vars_test    = predictive_vars[-inTrain,]
target_test             = target[-inTrain] 
predictive_vars_train   = predictive_vars[inTrain,]
target_train            = target[inTrain] 
```

### 3.3. Phase d'apprentissage

Le modèle d’arbre de classification est construit à l’aide du package
rpart, spécialement conçu pour la modélisation supervisée sur des
données catégorielles ou continues. On définit ici les paramètres de
contrôle du modèle : minsplit = 10 : nombre minimal d'observations dans
un nœud avant de tenter une séparation, cp = 0.001 : paramètre de
complexité pour éviter le surapprentissage, maxdepth = 5 : profondeur
maximale de l’arbre. Le modèle apprend à prédire la variable cible
(target_train) à partir de l’ensemble des variables explicatives
(predictive_vars_train).

Installation des packages `rpart`et `rpart.plot`

```{r, eval = FALSE}
install.packages("rpart")
install.packages("rpart.plot")
```

Chargement des librairies

```{r, results = "hide", message = FALSE}
library(rpart)
library(rpart.plot)
?rpart
?rpart.plot
```

### Entrainement d'un arbre de classification.

```{r, eval = FALSE}

predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_test <- as.data.frame(predictive_vars_test)

classifTree <- rpart(target_train ~ ., data = predictive_vars_train, 
                     method = "class",
                     control = rpart.control(minsplit = 5, cp = 0, maxdepth = 5))

```

Ce code entraine un arbre de classification. Il trouve les meilleurs
seuils de séparation pour classer correctement les observations.

### Visualisation de l'arbre de décision

```{r, eval = FALSE}
rpart.plot(classifTree, type = 3, extra = 3, fallen.leaves = TRUE,  
           main = "Arbre de prédiction du risque d'infarctus d'un patient", roundint=FALSE)  
```

### 3.4. Phase de test

```{r, eval = FALSE}

predictive_vars_test <- as.data.frame(predictive_vars_test)

classifTree <- rpart(target_test ~ ., data = predictive_vars_test, 
                     method = "class",
                     control = rpart.control(minsplit = 5, cp = 0, maxdepth = 5))
```

#### Visualisation de l'arbre

```{r}

rpart.plot(classifTree, type = 3, extra = 3, fallen.leaves = TRUE,  
           main = "Arbre de prédiction du risque d'infarctus d'un patient", roundint=FALSE)  

```

### 3.5. Prédiction de la target avec l'arbre de décision sur le jeu de données de test

```{r, eval = FALSE}
library(caret)
target_pred = predict(classifTree, predictive_vars_test, type = "class")
conf_matrix <- confusionMatrix(target_pred, target_test,positive = "1")
print(conf_matrix)
accuracy_test  <- conf_matrix$overall["Accuracy"]
recall_test    <- conf_matrix$byClass["Sensitivity"]  # Recall
precision_test <- conf_matrix$byClass["Precision"]
accuracy_test
recall_test
precision_test
# à compléter
# à compléter
# à compléter
```

### 3.6. Arbre de classification avec évaluation des performances en 10-fold cross-validation

## 4. La classification Naive bayésienne

### 4.1. Apprentissage du classifieur naïf bayésien

```{r, results = "hide", message = FALSE}
library(naivebayes)
?naive_bayes
```

```{r, eval = FALSE}
predictive_vars = cbind(quantitative_vars, qualitative_vars)
# Conversion de la target en facteur
set.seed(1357)  # Pour la reproductibilité
target
nobs = nrow(predictive_vars)
train_indices = sample(1:nobs, size = 0.8 * nobs)  # 80% des indices pour le train

# Création des ensembles Train/Test
predictive_vars_test  = predictive_vars[-inTrain, ]
predictive_vars_train=predictive_vars[inTrain,]
target_test  = target[-inTrain ]
predictive_vars_train <- as.data.frame(predictive_vars_train)
predictive_vars_test <- as.data.frame(predictive_vars_test)
```

```{r}
classif = naive_bayes(x = predictive_vars_train, y = target_train)
classif

```

#### 

```{r, out.width="50%", fig.align = "center", eval = FALSE}
plot(classif, prob = "conditional")
```

*Remarque :* Comme vu en cours, il est possible qu'une combinaison de
catégories pour la réponse et une variable explicative qualitative
X\^(l) n’apparaîsse pas dans jeu de données d’entraînement, auquel cas
on aurait hat P(X\^(l) = x\^(l) \| target) = 0. On peut alors utiliser
le additive smoothing (ou Laplace smoothing), ce qui se fait dans la
fonction `naive_bayes` avec l'argument `laplace`

### 4.2. Prédiction sur le jeu de données de test des probabilités d'appartenir aux différentes catégories de la réponse

```{r, eval = FALSE}
pred = predict(classif, predictive_vars_test, type = "prob")

```

### Prédiction de la target

```{r, eval = FALSE}
library(caret)

```

### Prédiction de la target sur le jeu de données de test

Utiliser le classifieur entraîné à la Partie 2.3 pour prédire la
catégorie d'appartenance des individus dans la partie Test, et calculer
l'accuracy, le recall, et la précision (à l'aide du package `caret`
comme fait aux TPs précédents, relire la documentation si besoin)

```{r, eval = FALSE}
library(caret)
library(Metrics)
#target_pred = predict(classif, predictive_vars_test)
#accuracy = mean(target_pred == target_test)
#recall_value = recall(target_pred, target_test)

target_pred = predict(classif, predictive_vars_test, type = "class")
conf_matrix <- confusionMatrix(target_pred, target_test, positive="1")
print(conf_matrix)
#precision_value = precision(target_pred, target_test,)


#mean(target_pred == # à compléter)
#recall(target, target_pred)
#precision(target, target_pred)
```
